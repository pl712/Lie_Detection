{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
    "\n",
    "from torchvision.models.inception import Inception3\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self, frame_length, encoding_length):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(frame_length, encoding_length)\n",
    "    \n",
    "    self.frame_length = frame_length\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    pe = self.embedding(torch.tensor([i for i in range(self.frame_length)]))\n",
    "\n",
    "    pe = pe[:x.shape[0]]\n",
    "\n",
    "    if len(x.shape) == 3:\n",
    "      self.pe = pe.unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
    "      x = torch.cat((x, pe), 2)\n",
    "    else:\n",
    "      x = torch.cat((x, pe), 1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_v3_url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "\n",
    "def inception_v3_sliced(stop_layer=3, **kwargs):\n",
    "\n",
    "    kwargs['transform_input'] = True\n",
    "    kwargs['init_weights'] = False\n",
    "\n",
    "    class Inception3Mod(Inception3):\n",
    "\n",
    "        def __init__(self, stop_layer, **kwargs):\n",
    "            super(Inception3Mod, self).__init__(**kwargs)\n",
    "            self.stop_layer = stop_layer\n",
    "            \n",
    "        def _forward(self, x):\n",
    "            layers = [\n",
    "            self.Conv2d_1a_3x3, self.Conv2d_2a_3x3, self.Conv2d_2b_3x3,\n",
    "            'maxpool',\n",
    "            self.Conv2d_3b_1x1, self.Conv2d_4a_3x3,\n",
    "            'maxpool',\n",
    "            self.Mixed_5b, self.Mixed_5c, self.Mixed_5d, self.Mixed_6a,\n",
    "            self.Mixed_6b, self.Mixed_6c, self.Mixed_6d, self.Mixed_6e,\n",
    "            self.Mixed_7a, self.Mixed_7b, self.Mixed_7c]\n",
    "\n",
    "            for idx in range(self.stop_layer):\n",
    "                layer = layers[idx]\n",
    "                if layer == 'maxpool':\n",
    "                    x = nn.functional.max_pool2d(x, kernel_size=3, stride=2)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "            return x, None\n",
    "        \n",
    "    model = Inception3Mod(**kwargs, stop_layer=stop_layer)\n",
    "    state_dict = load_state_dict_from_url(inc_v3_url, progress=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.aux_logits = False\n",
    "    del model.AuxLogits\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderTransformer(nn.Module):\n",
    "\n",
    "  def __init__(self, num_T_layers, num_fc_layers, outFeatCount, device, testLayer = 1, num_frames_max = 1000, stopLayer = 16, train = True, pos_encode_size = 10, n_hidden = 2048, n_heads = 86, dropout = 0.3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.posEncoder = positionalEncoder(num_frames_max, pos_encode_size)\n",
    "    \n",
    "    self.visionEncoder = inception_v3_sliced(stop_layer=stopLayer)\n",
    "    \n",
    "    self.transform = Compose([ToPILImage(), Resize((299, 299)), ToTensor()])\n",
    "\n",
    "    inFeatCount = 1280 + pos_encode_size\n",
    "    \n",
    "    encoder_layer = nn.TransformerEncoderLayer(inFeatCount, n_heads, n_hidden, dropout)\n",
    "    self.encoder = nn.TransformerEncoder(encoder_layer, num_T_layers)\n",
    "\n",
    "    self.fcLayers = []\n",
    "    currInput = inFeatCount\n",
    "\n",
    "    for i in range(num_fc_layers):\n",
    "      if i == num_fc_layers - 1:\n",
    "        self.fcLayers.append(nn.Linear(currInput, outFeatCount))\n",
    "      else:\n",
    "        self.fcLayers.append(nn.Linear(currInput, currInput // 2))\n",
    "        currInput = currInput // 2\n",
    "\n",
    "    print(f\"{len(self.fcLayers)} fc layers\")\n",
    "\n",
    "    self.device = device\n",
    "    self.train = train\n",
    "    \n",
    "    self.outputfc = min(testLayer, len(self.fcLayers))\n",
    "\n",
    "    print(f'output layer count: {self.outputfc}')\n",
    "    \n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "\n",
    "      for i in self.fcLayers:\n",
    "        i.bias.data.zero_()\n",
    "        i.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, data, vid = False, toCSV = False, toCSVName = None, toCSVPath = None):\n",
    "\n",
    "    arr = None\n",
    "    \n",
    "    if vid:\n",
    "      imgArr = []\n",
    "      vid = imageio.get_reader(data)\n",
    "      for i, im in enumerate(vid):\n",
    "        imgArr.append(im)\n",
    "    \n",
    "    elif not vid:\n",
    "      imgArr = data\n",
    "\n",
    "    for i in imgArr:\n",
    "      img = self.transform(i)[None]\n",
    "      picToVal = self.visionEncoder(img)[0]\n",
    "      to1D = nn.MaxPool2d((8, 8))(picToVal).squeeze()\n",
    "      if arr == None:\n",
    "        arr = to1D[None, :]\n",
    "      else:\n",
    "        arr = torch.cat((arr,to1D[None, :]), 0)\n",
    "\n",
    "    if vid: os.system(\"rm ./*.jpg\")\n",
    "  \n",
    "    encoded = self.posEncoder(arr) #make sure this works\n",
    "\n",
    "    result = self.encoder(encoded)\n",
    "\n",
    "    if self.train:\n",
    "      for i in self.fcLayers:\n",
    "        result = i(result)\n",
    "    else:\n",
    "      for i in range(min(self.outputfc, len(self.fcLayers))):\n",
    "        result = self.fcLayers[i](result)\n",
    "        if toCSV:\n",
    "          pd.DataFrame(result.numpy()).to_csv(f'{toCSVPath}{toCSVName}.csv')\n",
    "          return\n",
    "          \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openface = pd.read_csv(\"data/OpenFace/trial/lie/trial_lie_001.csv\")\n",
    "openface = openface[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x', 'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r'\\\n",
    ",'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "openface = torch.tensor(np.array(openface)).float()\n",
    "\n",
    "encoder_loss_fuction = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video:  trial_lie_041.mp4 finished:  0\n",
      "Processing video:  trial_lie_055.mp4 finished:  1\n",
      "Processing video:  trial_truth_056.mp4 finished:  2\n",
      "Processing video:  trial_truth_042.mp4 finished:  3\n",
      "Processing video:  trial_truth_043.mp4 finished:  4\n",
      "Processing video:  trial_truth_057.mp4 finished:  5\n",
      "Processing video:  trial_lie_054.mp4 finished:  6\n",
      "Processing video:  trial_lie_040.mp4 finished:  7\n",
      "Processing video:  trial_lie_056.mp4 finished:  8\n",
      "Processing video:  trial_lie_042.mp4 finished:  9\n"
     ]
    }
   ],
   "source": [
    "def preprocess(path):\n",
    "    imgArr = []\n",
    "    Openface_Arr = pd.DataFrame()\n",
    "    idx = 0\n",
    "    for video in os.listdir(path):\n",
    "        if video == 'trial_lie_043.mp4':\n",
    "            continue\n",
    "        if idx == 10:\n",
    "            break\n",
    "        if video.endswith(\".mp4\"):\n",
    "            print('Processing video: ', video, 'finished: ', idx)\n",
    "            vid = imageio.get_reader(path + \"/\" + video)\n",
    "            for frame_number, im in enumerate(vid):\n",
    "                im = cv2.resize(im, (240, 320))\n",
    "                imgArr.append(im)\n",
    "            \n",
    "            if video[:-4] + \".csv\" in os.listdir(\"data/OpenFace/trial/lie/\"):\n",
    "                file = pd.read_csv(\"data/OpenFace/trial/lie/\" + video[:-4] + \".csv\")\n",
    "                file = file[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x',\n",
    "                                'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r',\n",
    "                                'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "                Openface_Arr = pd.concat([Openface_Arr, file])\n",
    "            if video[:-4] + \".csv\" in os.listdir(\"data/OpenFace/trial/truth/\"):\n",
    "                file = pd.read_csv(\"data/OpenFace/trial/truth/\" + video[:-4] + \".csv\")\n",
    "                file = file[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x',\n",
    "                                'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r',\n",
    "                                'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "                Openface_Arr = pd.concat([Openface_Arr, file])\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    return imgArr, np.array(Openface_Arr)\n",
    "\n",
    "imgArr, Openface_Arr = preprocess(\"../Videos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(imgArr, Openface_Arr, test_size, random_state):\n",
    "    np.random.seed(random_state)\n",
    "    idx = np.random.permutation(len(imgArr))\n",
    "    test_size = int(len(imgArr) * test_size)\n",
    "    X_train = [imgArr[i] for i in idx[test_size:]]\n",
    "    X_test = [imgArr[i] for i in idx[:test_size]]\n",
    "    y_train = [Openface_Arr[i] for i in idx[test_size:]]\n",
    "    y_test = [Openface_Arr[i] for i in idx[:test_size]]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgArr, Openface_Arr, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fc layers\n",
      "output layer count: 1\n",
      "(10, 320, 240, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1605, -3.2836, -3.1442, -1.1042, -3.3615],\n",
       "        [ 0.6535,  0.7648,  1.5066, -3.0525,  0.4062],\n",
       "        [ 2.2584, -1.1254,  1.1361, -0.2209, -1.6487],\n",
       "        [ 1.6999,  1.6184, -2.1108, -0.5358, -2.8428],\n",
       "        [ 0.4091, -2.5580, -1.6994, -2.3198, -0.0530],\n",
       "        [-0.1001, -1.5891,  2.8875,  0.5293, -1.9649],\n",
       "        [ 3.9652, -2.5159,  0.2233,  0.6788, -2.2348],\n",
       "        [-0.2822, -1.3656,  0.7218,  0.0347, -1.9511],\n",
       "        [-0.2076, -1.6179, -0.8864, -3.5534, -1.2597],\n",
       "        [ 0.4455, -1.6718, -1.5639, -0.4150, -0.2399]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = encoderTransformer(1, 1, 5, 'cpu')\n",
    "\n",
    "print(np.array(X_train[0:10]).shape)\n",
    "model(X_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fc layers\n",
      "output layer count: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (15) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/finalized_code/Encoder.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             val_acc\u001b[39m.\u001b[39mappend(tot_acc\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(xtest))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m train_loss, val_loss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m train(model, X_train, y_train, X_test, y_test, \u001b[39m20\u001b[39;49m, \u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/finalized_code/Encoder.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, xtrain, ytrain, xtest, ytest, epochs, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(ytrain[i:\u001b[39mmin\u001b[39m(i\u001b[39m+\u001b[39mbatch_size, \u001b[39mlen\u001b[39m(ytrain))])\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m tot_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (y_pred\u001b[39m.\u001b[39;49margmax(\u001b[39m1\u001b[39;49m) \u001b[39m==\u001b[39;49m y)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(y_pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m tot_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (15) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model = encoderTransformer(1, 1, 15, 'cpu')\n",
    "\n",
    "def train(model, xtrain, ytrain, xtest, ytest, epochs, batch_size):\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        tot_loss = 0\n",
    "        tot_acc = 0\n",
    "        for i in range(0, len(xtrain), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            x = xtrain[i:min(i+batch_size, len(xtrain))]\n",
    "            y = torch.tensor(ytrain[i:min(i+batch_size, len(ytrain))]).float()\n",
    "            y_pred = model(x)\n",
    "            tot_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "            loss = loss_function(y_pred, y)\n",
    "            tot_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch: {_+1}, Train Loss: {tot_loss/len(xtrain)}, Train Accuracy: {tot_acc/len(xtrain)}\")\n",
    "        train_loss.append(tot_loss/len(xtrain))\n",
    "        train_acc.append(tot_acc/len(xtrain))\n",
    "\n",
    "        if _ % 10 == 0:\n",
    "            tot_loss = 0\n",
    "            test_acc = 0\n",
    "            for i in range(0, len(xtest), batch_size):\n",
    "                x = xtest[i:min(i+batch_size, len(xtest))]\n",
    "                y = torch.tensor(ytest[i:min(i+batch_size, len(ytest))]).float()\n",
    "                y_pred = model(x)\n",
    "                tot_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "                loss = loss_function(y_pred, y)\n",
    "                tot_loss += loss.item()\n",
    "            print(f\"Epoch: {_+1}, Validation Loss: {tot_loss/len(xtest)}, Validation Accuracy: {tot_acc/len(xtest)}\")\n",
    "            val_loss.append(tot_loss/len(xtest))\n",
    "            val_acc.append(tot_acc/len(xtest))\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "train(model, X_train, y_train, X_test, y_test, 20, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
