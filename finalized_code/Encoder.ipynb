{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
    "\n",
    "from torchvision.models.inception import Inception3\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self, frame_length, encoding_length):\n",
    "    super().__init__()\n",
    "\n",
    "    embedding = nn.Embedding(frame_length, encoding_length)\n",
    "    \n",
    "    self.pe = embedding(torch.tensor([i for i in range(frame_length)]))\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    self.pe = self.pe[:x.shape[0]]\n",
    "\n",
    "    if len(x.shape) == 3:\n",
    "      self.pe = self.pe.unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
    "      x = torch.cat((x, self.pe), 2)\n",
    "    else:\n",
    "      x = torch.cat((x, self.pe), 1)\n",
    "\n",
    "    return x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_v3_url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "\n",
    "def inception_v3_sliced(pretrained=True, progress=True, stop_layer=3, **kwargs):\n",
    "\n",
    "    kwargs['transform_input'] = True\n",
    "    kwargs['init_weights'] = False\n",
    "\n",
    "    class Inception3Mod(Inception3):\n",
    "\n",
    "        def __init__(self, stop_layer, **kwargs):\n",
    "            super(Inception3Mod, self).__init__(**kwargs)\n",
    "            self.stop_layer = stop_layer\n",
    "            \n",
    "        def _forward(self, x):\n",
    "            layers = [\n",
    "            self.Conv2d_1a_3x3, self.Conv2d_2a_3x3, self.Conv2d_2b_3x3,\n",
    "            'maxpool',\n",
    "            self.Conv2d_3b_1x1, self.Conv2d_4a_3x3,\n",
    "            'maxpool',\n",
    "            self.Mixed_5b, self.Mixed_5c, self.Mixed_5d, self.Mixed_6a,\n",
    "            self.Mixed_6b, self.Mixed_6c, self.Mixed_6d, self.Mixed_6e,\n",
    "            self.Mixed_7a, self.Mixed_7b, self.Mixed_7c]\n",
    "\n",
    "            for idx in range(self.stop_layer):\n",
    "                layer = layers[idx]\n",
    "                if layer == 'maxpool':\n",
    "                    x = nn.functional.max_pool2d(x, kernel_size=3, stride=2)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "            return x, None\n",
    "        \n",
    "    model = Inception3Mod(**kwargs, stop_layer=stop_layer)\n",
    "    state_dict = load_state_dict_from_url(inc_v3_url, progress=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.aux_logits = False\n",
    "    del model.AuxLogits\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderTransformer(nn.Module):\n",
    "\n",
    "  def __init__(self, num_T_layers, num_fc_layers, outFeatCount, device, testLayer = 1, num_frames_max = 1000, stopLayer = 16, train = True, pos_encode_size = 10, n_hidden = 2048, n_heads = 86, dropout = 0.3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.posEncoder = positionalEncoder(num_frames_max, pos_encode_size)\n",
    "    \n",
    "    self.visionEncoder = inception_v3_sliced(stop_layer=stopLayer)\n",
    "    \n",
    "    self.transform = Compose([ToPILImage(), Resize((299, 299)), ToTensor()])\n",
    "\n",
    "    inFeatCount = 1280 + pos_encode_size\n",
    "    \n",
    "    encoder_layer = nn.TransformerEncoderLayer(inFeatCount, n_heads, n_hidden, dropout)\n",
    "    self.encoder = nn.TransformerEncoder(encoder_layer, num_T_layers)\n",
    "\n",
    "    self.fcLayers = []\n",
    "    currInput = inFeatCount\n",
    "\n",
    "    for i in range(num_fc_layers):\n",
    "      if i == num_fc_layers - 1:\n",
    "        self.fcLayers.append(nn.Linear(currInput, outFeatCount))\n",
    "      else:\n",
    "        self.fcLayers.append(nn.Linear(currInput, currInput // 2))\n",
    "        currInput = currInput // 2\n",
    "\n",
    "    print(f\"{len(self.fcLayers)} fc layers\")\n",
    "\n",
    "    self.device = device\n",
    "    self.train = train\n",
    "    \n",
    "    self.outputfc = min(testLayer, len(self.fcLayers))\n",
    "\n",
    "    print(f'output layer count: {self.outputfc}')\n",
    "    \n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "\n",
    "      for i in self.fcLayers:\n",
    "        i.bias.data.zero_()\n",
    "        i.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, data, vid = False):\n",
    "\n",
    "    arr = None\n",
    "    \n",
    "    if vid:\n",
    "      imgArr = []\n",
    "      vid = imageio.get_reader(data)\n",
    "      for i, im in enumerate(vid):\n",
    "        imgArr.append(im)\n",
    "    \n",
    "    elif not vid:\n",
    "      imgArr = data\n",
    "\n",
    "    for i in imgArr:\n",
    "      img = self.transform(i)[None]\n",
    "      picToVal = self.visionEncoder(img)[0]\n",
    "      to1D = nn.MaxPool2d((8, 8))(picToVal).squeeze()\n",
    "      if arr == None:\n",
    "        arr = to1D[None, :]\n",
    "      else:\n",
    "        arr = torch.cat((arr,to1D[None, :]), 0)\n",
    "\n",
    "    if vid: os.system(\"rm ./*.jpg\")\n",
    "  \n",
    "    encoded = self.posEncoder(torch.Tensor(arr))\n",
    "\n",
    "    result = self.encoder(encoded)\n",
    "\n",
    "    if self.train:\n",
    "      for i in self.fcLayers:\n",
    "        result = i(result)\n",
    "    else:\n",
    "      for i in range(min(self.outputfc, len(self.fcLayers))):\n",
    "        result = self.fcLayers[i](result)\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openface = pd.read_csv(\"processed_lie/trial_lie_001.csv\")\n",
    "openface = openface[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x', 'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r'\\\n",
    ",'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "openface = torch.tensor(np.array(openface)).float()\n",
    "\n",
    "encoder_loss_fuction = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    imgArr = []\n",
    "    Openface_Arr = pd.DataFrame()\n",
    "    idx = 0\n",
    "    for video in os.listdir(path):\n",
    "        if video == 'trial_lie_043.mp4':\n",
    "            continue\n",
    "        if idx == 10:\n",
    "            break\n",
    "        if video.endswith(\".mp4\"):\n",
    "            print('Processing video: ', video, 'finished: ', idx)\n",
    "            vid = imageio.get_reader(path + \"/\" + video)\n",
    "            for frame_number, im in enumerate(vid):\n",
    "                im = cv2.resize(im, (240, 320))\n",
    "                imgArr.append(im)\n",
    "            \n",
    "            if video[:-4] + \".csv\" in os.listdir(\"processed_lie\"):\n",
    "                file = pd.read_csv(\"processed_lie/\" + video[:-4] + \".csv\")\n",
    "                file = file[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x',\n",
    "                                'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r',\n",
    "                                'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "                Openface_Arr = pd.concat([Openface_Arr, file])\n",
    "            if video[:-4] + \".csv\" in os.listdir(\"processed_truth\"):\n",
    "                file = pd.read_csv(\"processed_truth/\" + video[:-4] + \".csv\")\n",
    "                file = file[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x',\n",
    "                                'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r',\n",
    "                                'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "                Openface_Arr = pd.concat([Openface_Arr, file])\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    return imgArr, np.array(Openface_Arr)\n",
    "\n",
    "imgArr, Openface_Arr = preprocess(\"videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(imgArr, Openface_Arr, test_size, random_state):\n",
    "    np.random.seed(random_state)\n",
    "    idx = np.random.permutation(len(imgArr))\n",
    "    test_size = int(len(imgArr) * test_size)\n",
    "    X_train = [imgArr[i] for i in idx[test_size:]]\n",
    "    X_test = [imgArr[i] for i in idx[:test_size]]\n",
    "    y_train = [Openface_Arr[i] for i in idx[test_size:]]\n",
    "    y_test = [Openface_Arr[i] for i in idx[:test_size]]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgArr, Openface_Arr, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoderTransformer(1, 1, 15, 'cpu')\n",
    "\n",
    "def train(model, xtrain, ytrain, xtest, ytest, epochs, batch_size):\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for i in range(0, len(xtrain), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            x = xtrain[i:min(i+batch_size, len(xtrain))]\n",
    "            y = torch.tensor(ytrain[i:min(i+batch_size, len(ytrain))]).float()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_function(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "train(model, X_train, y_train, X_test, y_test, 20, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
