{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
    "\n",
    "from torchvision.models.inception import Inception3\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self, frame_length, encoding_length):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(frame_length, encoding_length)\n",
    "    \n",
    "    self.frame_length = frame_length\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    pe = self.embedding(torch.tensor([i for i in range(self.frame_length)]))\n",
    "\n",
    "    pe = pe[:x.shape[0]]\n",
    "\n",
    "    if len(x.shape) == 3:\n",
    "      self.pe = pe.unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
    "      x = torch.cat((x, pe), 2)\n",
    "    else:\n",
    "      x = torch.cat((x, pe), 1)\n",
    "\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_v3_url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "\n",
    "def inception_v3_sliced(stop_layer=3, **kwargs):\n",
    "\n",
    "    kwargs['transform_input'] = True\n",
    "    kwargs['init_weights'] = False\n",
    "\n",
    "    class Inception3Mod(Inception3):\n",
    "\n",
    "        def __init__(self, stop_layer, **kwargs):\n",
    "            super(Inception3Mod, self).__init__(**kwargs)\n",
    "            self.stop_layer = stop_layer\n",
    "            \n",
    "        def _forward(self, x):\n",
    "            layers = [\n",
    "            self.Conv2d_1a_3x3, self.Conv2d_2a_3x3, self.Conv2d_2b_3x3,\n",
    "            'maxpool',\n",
    "            self.Conv2d_3b_1x1, self.Conv2d_4a_3x3,\n",
    "            'maxpool',\n",
    "            self.Mixed_5b, self.Mixed_5c, self.Mixed_5d, self.Mixed_6a,\n",
    "            self.Mixed_6b, self.Mixed_6c, self.Mixed_6d, self.Mixed_6e,\n",
    "            self.Mixed_7a, self.Mixed_7b, self.Mixed_7c]\n",
    "\n",
    "            for idx in range(self.stop_layer):\n",
    "                layer = layers[idx]\n",
    "                if layer == 'maxpool':\n",
    "                    x = nn.functional.max_pool2d(x, kernel_size=3, stride=2)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "            return x, None\n",
    "        \n",
    "    model = Inception3Mod(**kwargs, stop_layer=stop_layer)\n",
    "    state_dict = load_state_dict_from_url(inc_v3_url, progress=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.aux_logits = False\n",
    "    del model.AuxLogits\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderTransformer(nn.Module):\n",
    "\n",
    "  def __init__(self, num_T_layers, num_fc_layers, outFeatCount, device, testLayer = 1, num_frames_max = 1000, stopLayer = 16, train = True, pos_encode_size = 10, n_hidden = 2048, n_heads = 86, dropout = 0.3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.posEncoder = positionalEncoder(num_frames_max, pos_encode_size)\n",
    "    \n",
    "    self.visionEncoder = inception_v3_sliced(stop_layer=stopLayer)\n",
    "    \n",
    "    self.transform = Compose([ToPILImage(), Resize((299, 299)), ToTensor()])\n",
    "\n",
    "    inFeatCount = 1280 + pos_encode_size\n",
    "    \n",
    "    encoder_layer = nn.TransformerEncoderLayer(inFeatCount, n_heads, n_hidden, dropout)\n",
    "    self.encoder = nn.TransformerEncoder(encoder_layer, num_T_layers)\n",
    "\n",
    "    self.fcLayers = []\n",
    "    currInput = inFeatCount\n",
    "\n",
    "    for i in range(num_fc_layers):\n",
    "      if i == num_fc_layers - 1:\n",
    "        self.fcLayers.append(nn.Linear(currInput, outFeatCount))\n",
    "      else:\n",
    "        self.fcLayers.append(nn.Linear(currInput, currInput // 2))\n",
    "        currInput = currInput // 2\n",
    "\n",
    "    print(f\"{len(self.fcLayers)} fc layers\")\n",
    "\n",
    "    self.device = device\n",
    "    self.train = train\n",
    "    \n",
    "    self.outputfc = min(testLayer, len(self.fcLayers))\n",
    "\n",
    "    print(f'output layer count: {self.outputfc}')\n",
    "    \n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "\n",
    "      for i in self.fcLayers:\n",
    "        i.bias.data.zero_()\n",
    "        i.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, data, vid = False, toCSV = False, toCSVName = None, toCSVPath = None):\n",
    "\n",
    "    arr = None\n",
    "    \n",
    "    if vid:\n",
    "      imgArr = []\n",
    "      vid = imageio.get_reader(data)\n",
    "      for i, im in enumerate(vid):\n",
    "        imgArr.append(im)\n",
    "    \n",
    "    elif not vid:\n",
    "      imgArr = data\n",
    "\n",
    "    for i in imgArr:\n",
    "      img = self.transform(i)[None]\n",
    "      picToVal = self.visionEncoder(img)[0]\n",
    "      to1D = nn.MaxPool2d((8, 8))(picToVal).squeeze()\n",
    "      if arr == None:\n",
    "        arr = to1D[None, :]\n",
    "      else:\n",
    "        arr = torch.cat((arr,to1D[None, :]), 0)\n",
    "\n",
    "    if vid: os.system(\"rm ./*.jpg\")\n",
    "  \n",
    "    encoded = self.posEncoder(arr) #make sure this works\n",
    "\n",
    "    result = self.encoder(encoded)\n",
    "\n",
    "    if self.train:\n",
    "      for i in self.fcLayers:\n",
    "        result = i(result)\n",
    "    else:\n",
    "      for i in range(min(self.outputfc, len(self.fcLayers))):\n",
    "        result = self.fcLayers[i](result)\n",
    "        if toCSV:\n",
    "          pd.DataFrame(result.numpy()).to_csv(f'{toCSVPath}{toCSVName}.csv')\n",
    "          return\n",
    "          \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openface = pd.read_csv(\"data/OpenFace/trial/lie/trial_lie_001.csv\")\n",
    "openface = openface[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x', 'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r'\\\n",
    ",'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "openface = torch.tensor(np.array(openface)).float()\n",
    "\n",
    "encoder_loss_fuction = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video:  trial_lie_041.mp4 finished:  0\n",
      "Processing video:  trial_lie_055.mp4 finished:  1\n",
      "Processing video:  trial_truth_056.mp4 finished:  2\n",
      "Processing video:  trial_truth_042.mp4 finished:  3\n",
      "Processing video:  trial_truth_043.mp4 finished:  4\n",
      "Processing video:  trial_truth_057.mp4 finished:  5\n",
      "Processing video:  trial_lie_054.mp4 finished:  6\n",
      "Processing video:  trial_lie_040.mp4 finished:  7\n",
      "Processing video:  trial_lie_056.mp4 finished:  8\n",
      "Processing video:  trial_lie_042.mp4 finished:  9\n"
     ]
    }
   ],
   "source": [
    "def preprocess(path):\n",
    "    imgArr = []\n",
    "    Openface_Arr = pd.DataFrame()\n",
    "    idx = 0\n",
    "    for video in os.listdir(path):\n",
    "        if video == 'trial_lie_043.mp4':\n",
    "            continue\n",
    "        if idx == 10:\n",
    "            break\n",
    "        if video.endswith(\".mp4\"):\n",
    "            print('Processing video: ', video, 'finished: ', idx)\n",
    "            vid = imageio.get_reader(path + \"/\" + video)\n",
    "            for frame_number, im in enumerate(vid):\n",
    "                im = cv2.resize(im, (240, 320))\n",
    "                imgArr.append(im)\n",
    "            \n",
    "            if video[:-4] + \".csv\" in os.listdir(\"data/OpenFace/trial/lie/\"):\n",
    "                file = pd.read_csv(\"data/OpenFace/trial/lie/\" + video[:-4] + \".csv\")\n",
    "                file = file[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x',\n",
    "                                'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r',\n",
    "                                'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "                Openface_Arr = pd.concat([Openface_Arr, file])\n",
    "            if video[:-4] + \".csv\" in os.listdir(\"data/OpenFace/trial/truth/\"):\n",
    "                file = pd.read_csv(\"data/OpenFace/trial/truth/\" + video[:-4] + \".csv\")\n",
    "                file = file[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x',\n",
    "                                'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r',\n",
    "                                'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "                Openface_Arr = pd.concat([Openface_Arr, file])\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    return imgArr, np.array(Openface_Arr)\n",
    "\n",
    "imgArr, Openface_Arr = preprocess(\"../Videos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(imgArr, Openface_Arr, test_size, random_state):\n",
    "    np.random.seed(random_state)\n",
    "    idx = np.random.permutation(len(imgArr))\n",
    "    test_size = int(len(imgArr) * test_size)\n",
    "    X_train = [imgArr[i] for i in idx[test_size:]]\n",
    "    X_test = [imgArr[i] for i in idx[:test_size]]\n",
    "    y_train = [Openface_Arr[i] for i in idx[test_size:]]\n",
    "    y_test = [Openface_Arr[i] for i in idx[:test_size]]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgArr, Openface_Arr, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fc layers\n",
      "output layer count: 1\n",
      "(10, 320, 240, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-7.5498e-02, -2.8750e+00,  1.8817e+00,  1.0084e+00,  6.1173e-01,\n",
       "         -2.6245e+00,  3.3684e-01, -4.7728e-01,  2.2174e+00,  9.6956e-01,\n",
       "         -1.3995e+00,  1.1406e+00,  5.3648e-01, -1.1025e+00,  2.4141e+00],\n",
       "        [-1.9975e+00,  3.0389e+00, -1.4600e+00, -1.0495e+00,  1.7765e+00,\n",
       "         -1.2133e+00, -1.7972e+00,  3.0130e+00, -2.6052e-01,  2.8389e+00,\n",
       "          1.2919e+00,  3.6225e+00, -2.0970e+00, -3.9113e-03,  2.6600e-01],\n",
       "        [ 3.8864e-01, -9.5516e-01,  1.1841e+00,  1.7247e-01,  2.7511e-01,\n",
       "         -4.2051e-01, -9.5092e-01,  2.4355e+00,  1.2049e+00,  3.6847e+00,\n",
       "          3.3409e-01,  9.1909e-01, -2.0130e+00, -2.7060e+00,  3.7502e+00],\n",
       "        [ 1.4020e+00, -2.8604e-01,  5.1584e-01, -2.3447e+00,  2.2565e-02,\n",
       "          3.8842e-01,  4.7095e-02,  3.3504e+00,  1.0034e+00,  2.0034e+00,\n",
       "          1.2500e+00,  2.2443e+00,  4.8683e-01, -3.2714e+00,  1.1672e+00],\n",
       "        [-1.0307e+00, -1.5193e+00, -2.1108e+00, -2.0884e+00,  8.2929e-01,\n",
       "         -2.8127e+00,  7.8190e-01,  2.7382e+00, -6.5722e-01,  2.6247e+00,\n",
       "         -1.9757e+00,  4.8315e-02,  4.4564e-01,  1.3151e+00,  2.7810e+00],\n",
       "        [-1.5585e+00,  2.2593e+00,  9.6258e-01,  1.1136e-01,  5.9261e-01,\n",
       "         -3.2867e-01,  2.7174e+00,  1.0052e+00,  9.3104e-01,  2.3547e+00,\n",
       "         -1.7656e-01,  2.4793e+00, -1.9002e+00, -3.3206e-02,  2.7022e+00],\n",
       "        [ 1.8436e-01, -7.7827e-01,  6.2067e-01, -2.3876e+00,  2.7455e+00,\n",
       "         -3.0208e-01,  2.4394e+00,  7.4678e-01, -5.1695e-02,  2.6429e+00,\n",
       "          1.6170e+00,  1.7220e+00,  2.3634e+00, -1.3221e+00,  1.2877e+00],\n",
       "        [ 1.6909e+00, -2.4918e-01, -4.2099e-04,  3.4065e-02, -2.3501e+00,\n",
       "          1.4009e+00, -9.8904e-01, -1.7944e-01,  5.9919e-01, -6.7700e-01,\n",
       "          1.5765e+00,  1.1692e+00, -2.9359e-01, -4.3930e-01,  1.1489e+00],\n",
       "        [-7.4768e-02,  1.5674e-01, -6.6963e-01, -1.6200e+00,  1.8156e+00,\n",
       "         -1.5378e+00,  8.3589e-01,  2.3837e+00,  4.0953e+00,  2.4055e+00,\n",
       "         -6.1221e-01,  2.8386e+00, -8.5655e-01, -5.9210e-01,  8.7003e-01],\n",
       "        [ 2.4837e+00,  1.2371e+00,  1.6119e+00, -1.8617e-01,  2.1260e+00,\n",
       "         -4.2982e+00, -6.2001e-01,  3.1165e+00,  8.0895e-01,  2.0720e+00,\n",
       "          1.6838e+00,  2.3118e+00, -2.0472e+00, -2.0709e+00,  1.1904e+00]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = encoderTransformer(1, 1, 15, 'cpu')\n",
    "\n",
    "print(np.array(X_train[0:10]).shape)\n",
    "model(X_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fc layers\n",
      "output layer count: 1\n"
     ]
    }
   ],
   "source": [
    "model = encoderTransformer(1, 1, 15, 'cpu')\n",
    "\n",
    "def train(model, xtrain, ytrain, xtest, ytest, epochs, batch_size):\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        tot_loss = 0\n",
    "        for i in range(0, len(xtrain), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            x = xtrain[i:min(i+batch_size, len(xtrain))]\n",
    "            y = torch.tensor(ytrain[i:min(i+batch_size, len(ytrain))]).float()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_function(y_pred, y)\n",
    "            tot_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch: {_+1}, Train Loss: {tot_loss/len(xtrain)}\")\n",
    "        train_loss.append(tot_loss/len(xtrain))\n",
    "\n",
    "        if _ % 10 == 0:\n",
    "            tot_loss = 0\n",
    "            for i in range(0, len(xtest), batch_size):\n",
    "                x = xtest[i:min(i+batch_size, len(xtest))]\n",
    "                y = torch.tensor(ytest[i:min(i+batch_size, len(ytest))]).float()\n",
    "                y_pred = model(x)\n",
    "                loss = loss_function(y_pred, y)\n",
    "                tot_loss += loss.item()\n",
    "            print(f\"Epoch: {_+1}, Validation Loss: {tot_loss/len(xtest)}\")\n",
    "            print(\"--------------------------------------------------\")\n",
    "            print(\"Real label:\" + str(y[0]))\n",
    "            print(\"Predicted label:\" + str(y_pred[0]))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            val_loss.append(tot_loss/len(xtest))\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "train(model, X_train, y_train, X_test, y_test, 20, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
