{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
    "\n",
    "from torchvision.models.inception import Inception3\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self, frame_length, encoding_length):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(frame_length, encoding_length)\n",
    "    \n",
    "    self.frame_length = frame_length\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    pe = self.embedding(torch.tensor([i for i in range(self.frame_length)]))\n",
    "\n",
    "    pe = pe[:x.shape[0]]\n",
    "\n",
    "    if len(x.shape) == 3:\n",
    "      self.pe = pe.unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
    "      x = torch.cat((x, pe), 2)\n",
    "    else:\n",
    "      x = torch.cat((x, pe), 1)\n",
    "\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_v3_url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "\n",
    "def inception_v3_sliced(stop_layer=3, **kwargs):\n",
    "\n",
    "    kwargs['transform_input'] = True\n",
    "    kwargs['init_weights'] = False\n",
    "\n",
    "    class Inception3Mod(Inception3):\n",
    "\n",
    "        def __init__(self, stop_layer, **kwargs):\n",
    "            super(Inception3Mod, self).__init__(**kwargs)\n",
    "            self.stop_layer = stop_layer\n",
    "            \n",
    "        def _forward(self, x):\n",
    "            layers = [\n",
    "            self.Conv2d_1a_3x3, self.Conv2d_2a_3x3, self.Conv2d_2b_3x3,\n",
    "            'maxpool',\n",
    "            self.Conv2d_3b_1x1, self.Conv2d_4a_3x3,\n",
    "            'maxpool',\n",
    "            self.Mixed_5b, self.Mixed_5c, self.Mixed_5d, self.Mixed_6a,\n",
    "            self.Mixed_6b, self.Mixed_6c, self.Mixed_6d, self.Mixed_6e,\n",
    "            self.Mixed_7a, self.Mixed_7b, self.Mixed_7c]\n",
    "\n",
    "            for idx in range(self.stop_layer):\n",
    "                layer = layers[idx]\n",
    "                if layer == 'maxpool':\n",
    "                    x = nn.functional.max_pool2d(x, kernel_size=3, stride=2)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "            return x, None\n",
    "        \n",
    "    model = Inception3Mod(**kwargs, stop_layer=stop_layer)\n",
    "    state_dict = load_state_dict_from_url(inc_v3_url, progress=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.aux_logits = False\n",
    "    del model.AuxLogits\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderTransformer(nn.Module):\n",
    "\n",
    "  def __init__(self, num_T_layers, num_fc_layers, outFeatCount, device, testLayer = 1, num_frames_max = 1000, stopLayer = 16, train = True, pos_encode_size = 10, n_hidden = 2048, n_heads = 86, dropout = 0.3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.posEncoder = positionalEncoder(num_frames_max, pos_encode_size)\n",
    "    \n",
    "    self.visionEncoder = inception_v3_sliced(stop_layer=stopLayer)\n",
    "    \n",
    "    self.transform = Compose([ToPILImage(), Resize((299, 299)), ToTensor()])\n",
    "\n",
    "    inFeatCount = 1280 + pos_encode_size\n",
    "    \n",
    "    encoder_layer = nn.TransformerEncoderLayer(inFeatCount, n_heads, n_hidden, dropout)\n",
    "    self.encoder = nn.TransformerEncoder(encoder_layer, num_T_layers)\n",
    "\n",
    "    self.fcLayers = []\n",
    "    currInput = inFeatCount\n",
    "\n",
    "    for i in range(num_fc_layers):\n",
    "      if i == num_fc_layers - 1:\n",
    "        self.fcLayers.append(nn.Linear(currInput, outFeatCount))\n",
    "      else:\n",
    "        self.fcLayers.append(nn.Linear(currInput, currInput // 2))\n",
    "        currInput = currInput // 2\n",
    "\n",
    "    print(f\"{len(self.fcLayers)} fc layers\")\n",
    "\n",
    "    self.device = device\n",
    "    self.train = train\n",
    "    \n",
    "    self.outputfc = min(testLayer, len(self.fcLayers))\n",
    "\n",
    "    print(f'output layer count: {self.outputfc}')\n",
    "    \n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "\n",
    "      for i in self.fcLayers:\n",
    "        i.bias.data.zero_()\n",
    "        i.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, data, vid = False, toCSV = False, toCSVName = None, toCSVPath = None):\n",
    "\n",
    "    arr = None\n",
    "    \n",
    "    if vid:\n",
    "      imgArr = []\n",
    "      vid = imageio.get_reader(data)\n",
    "      for i, im in enumerate(vid):\n",
    "        imgArr.append(im)\n",
    "    \n",
    "    elif not vid:\n",
    "      imgArr = data\n",
    "\n",
    "    for i in imgArr:\n",
    "      img = self.transform(i)[None]\n",
    "      picToVal = self.visionEncoder(img)[0]\n",
    "      to1D = nn.MaxPool2d((8, 8))(picToVal).squeeze()\n",
    "      if arr == None:\n",
    "        arr = to1D[None, :]\n",
    "      else:\n",
    "        arr = torch.cat((arr,to1D[None, :]), 0)\n",
    "\n",
    "    if vid: os.system(\"rm ./*.jpg\")\n",
    "  \n",
    "    encoded = self.posEncoder(arr) #make sure this works\n",
    "\n",
    "    result = self.encoder(encoded)\n",
    "\n",
    "    if self.train:\n",
    "      for i in self.fcLayers:\n",
    "        result = i(result)\n",
    "    else:\n",
    "      for i in range(min(self.outputfc, len(self.fcLayers))):\n",
    "        result = self.fcLayers[i](result)\n",
    "        if toCSV:\n",
    "          pd.DataFrame(result.numpy()).to_csv(f'{toCSVPath}{toCSVName}.csv')\n",
    "          return\n",
    "          \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openface = pd.read_csv(\"data/OpenFace/trial/lie/trial_lie_001.csv\")\n",
    "openface = openface[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x', 'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r'\\\n",
    ",'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "openface = torch.tensor(np.array(openface)).float()\n",
    "\n",
    "encoder_loss_fuction = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video:  trial_lie_041.mp4 finished:  0\n",
      "Processing video:  trial_lie_055.mp4 finished:  1\n",
      "Processing video:  trial_truth_056.mp4 finished:  2\n",
      "Processing video:  trial_truth_042.mp4 finished:  3\n",
      "Processing video:  trial_truth_043.mp4 finished:  4\n",
      "Processing video:  trial_truth_057.mp4 finished:  5\n",
      "Processing video:  trial_lie_054.mp4 finished:  6\n",
      "Processing video:  trial_lie_040.mp4 finished:  7\n",
      "Processing video:  trial_lie_056.mp4 finished:  8\n",
      "Processing video:  trial_lie_042.mp4 finished:  9\n"
     ]
    }
   ],
   "source": [
    "def preprocess(path):\n",
    "    imgArr = []\n",
    "    Openface_Arr = pd.DataFrame()\n",
    "    idx = 0\n",
    "    for video in os.listdir(path):\n",
    "        if video == 'trial_lie_043.mp4':\n",
    "            continue\n",
    "        if idx == 10:\n",
    "            break\n",
    "        if video.endswith(\".mp4\"):\n",
    "            print('Processing video: ', video, 'finished: ', idx)\n",
    "            vid = imageio.get_reader(path + \"/\" + video)\n",
    "            for frame_number, im in enumerate(vid):\n",
    "                im = cv2.resize(im, (240, 320))\n",
    "                imgArr.append(im)\n",
    "            \n",
    "            if video[:-4] + \".csv\" in os.listdir(\"data/OpenFace/trial/lie/\"):\n",
    "                file = pd.read_csv(\"data/OpenFace/trial/lie/\" + video[:-4] + \".csv\")\n",
    "                file = file[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x',\n",
    "                                'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r',\n",
    "                                'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "                Openface_Arr = pd.concat([Openface_Arr, file])\n",
    "            if video[:-4] + \".csv\" in os.listdir(\"data/OpenFace/trial/truth/\"):\n",
    "                file = pd.read_csv(\"data/OpenFace/trial/truth/\" + video[:-4] + \".csv\")\n",
    "                file = file[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x',\n",
    "                                'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r',\n",
    "                                'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "                Openface_Arr = pd.concat([Openface_Arr, file])\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    return imgArr, np.array(Openface_Arr)\n",
    "\n",
    "imgArr, Openface_Arr = preprocess(\"../Videos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(imgArr, Openface_Arr, test_size, random_state):\n",
    "    np.random.seed(random_state)\n",
    "    idx = np.random.permutation(len(imgArr))\n",
    "    test_size = int(len(imgArr) * test_size)\n",
    "    X_train = [imgArr[i] for i in idx[test_size:]]\n",
    "    X_test = [imgArr[i] for i in idx[:test_size]]\n",
    "    y_train = [Openface_Arr[i] for i in idx[test_size:]]\n",
    "    y_test = [Openface_Arr[i] for i in idx[:test_size]]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgArr, Openface_Arr, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fc layers\n",
      "output layer count: 1\n",
      "(10, 320, 240, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3334, -0.6740, -0.9687,  0.6636,  0.1556,  1.0081, -1.2435,  0.1888,\n",
       "         -3.2547, -0.8482,  0.5522,  0.5798,  0.4512, -0.5924, -2.5648],\n",
       "        [ 3.2803,  0.8815, -1.3916,  1.1645,  3.5791, -0.4872, -2.6105, -0.1547,\n",
       "         -2.1169,  3.3896,  0.4593,  1.9007, -1.0901,  3.8276, -1.6141],\n",
       "        [ 2.2881,  0.0915,  1.7063,  0.3307,  0.4165,  0.4064,  0.2147,  1.9171,\n",
       "          0.5738, -0.0935, -0.4436,  2.0648, -1.6303,  0.7222,  0.3801],\n",
       "        [ 0.9102, -2.3913,  0.2048, -1.0383,  0.8094, -0.8167, -1.0226,  1.6954,\n",
       "          0.9251, -0.2036, -0.2613, -0.1793, -1.8403,  1.4284,  0.7816],\n",
       "        [ 4.1569, -0.3388, -1.3424,  0.2793,  1.2752, -0.6394, -0.7906, -0.5279,\n",
       "         -1.2171,  1.3674, -0.4504, -1.7296, -1.8088,  0.5283, -2.3353],\n",
       "        [ 3.7187,  2.8601,  0.5067,  0.2309,  1.7436,  0.8203, -2.3202,  0.7853,\n",
       "         -1.0152, -0.0242,  2.1018,  4.1005, -2.0456,  1.4224,  1.6945],\n",
       "        [ 2.6632,  1.1737, -2.1844, -1.3761,  0.3152,  0.3916, -1.1029,  1.5471,\n",
       "         -1.2221,  1.9952, -0.1549,  1.1696, -2.3897, -0.3780, -0.7739],\n",
       "        [ 2.9610,  2.0737,  1.5198,  1.5491, -0.1928,  1.4692, -2.3208,  0.4439,\n",
       "         -1.8390,  0.2728, -0.9298,  0.8635, -1.7005,  1.2963, -0.4010],\n",
       "        [ 5.3006,  2.6224, -2.3067,  2.4494,  5.8246,  1.2352, -1.0303,  0.8033,\n",
       "         -1.1185,  0.8791, -0.3995,  1.1096, -3.8472,  0.5677,  1.8158],\n",
       "        [ 0.8720,  1.9412,  0.0367, -0.2477,  0.3772, -0.4667, -1.2014,  1.7900,\n",
       "          1.5719, -3.0297,  0.3638,  2.5146, -1.3998, -0.9102,  0.1951]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = encoderTransformer(1, 1, 15, 'cpu')\n",
    "\n",
    "print(np.array(X_train[0:10]).shape)\n",
    "model(X_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fc layers\n",
      "output layer count: 1\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n",
      "I am here\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/finalized_code/Encoder.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m             val_loss\u001b[39m.\u001b[39mappend(tot_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(xtest))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m train_loss, val_loss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m train(model, X_train, y_train, X_test, y_test, \u001b[39m20\u001b[39;49m, \u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/finalized_code/Encoder.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, xtrain, ytrain, xtest, ytest, epochs, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(y_pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m tot_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/finalized_code/Encoder.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mI am here\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = encoderTransformer(1, 1, 15, 'cpu')\n",
    "\n",
    "def train(model, xtrain, ytrain, xtest, ytest, epochs, batch_size):\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        tot_loss = 0\n",
    "        for i in range(0, len(xtrain), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            x = xtrain[i:min(i+batch_size, len(xtrain))]\n",
    "            y = torch.tensor(ytrain[i:min(i+batch_size, len(ytrain))]).float()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_function(y_pred, y)\n",
    "            tot_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"I am here\")\n",
    "\n",
    "        print(f\"Epoch: {_+1}, Train Loss: {tot_loss/len(xtrain)}\")\n",
    "        train_loss.append(tot_loss/len(xtrain))\n",
    "\n",
    "        if _ % 10 == 0:\n",
    "            tot_loss = 0\n",
    "            for i in range(0, len(xtest), batch_size):\n",
    "                x = xtest[i:min(i+batch_size, len(xtest))]\n",
    "                y = torch.tensor(ytest[i:min(i+batch_size, len(ytest))]).float()\n",
    "                y_pred = model(x)\n",
    "                loss = loss_function(y_pred, y)\n",
    "                tot_loss += loss.item()\n",
    "            print(f\"Epoch: {_+1}, Validation Loss: {tot_loss/len(xtest)}\")\n",
    "            print(\"--------------------------------------------------\")\n",
    "            print(\"Real label:\" + str(y[0]))\n",
    "            print(\"Predicted label:\" + str(y_pred[0]))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            val_loss.append(tot_loss/len(xtest))\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "train(model, X_train, y_train, X_test, y_test, 20, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
