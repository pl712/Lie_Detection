{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import sklearn.metrics as sk\n",
    "# import tensorflow_decision_forests as tfdf\n",
    "\n",
    "featuresToKeep = [\"gaze_0_x\",\"gaze_0_y\",\"gaze_0_z\",\"gaze_angle_x\", \"gaze_angle_y\",\n",
    "                  \"AU01_r\",\"AU04_r\",\"AU10_r\",\"AU12_r\",\"AU45_r\", \n",
    "                  \"pose_Tx\", \"pose_Ty\", \"pose_Tz\", \"pose_Ry\", \n",
    "                  \"Result\", \"confidence\", \"Person\"]\n",
    "\n",
    "def getLSTMBlocks(inputLst, dataLength, blockSize = 10, start = 0):\n",
    "  inputLst.sort()\n",
    "  \n",
    "  listOfRanges = []\n",
    "  \n",
    "  inputLst.append(dataLength)\n",
    "\n",
    "  while inputLst:\n",
    "    if (start + blockSize - 1) < inputLst[0]:\n",
    "      listOfRanges.append([start, start + blockSize - 1])\n",
    "      start += 1\n",
    "    else:\n",
    "      start = inputLst[0] + 1\n",
    "      inputLst.pop(0)\n",
    "\n",
    "  return listOfRanges\n",
    "\n",
    "def shuffleByPerson(df, ratio = 0.2, lst = []):\n",
    "\n",
    "    if lst == []:\n",
    "        df = df.sort_values(by=['Person']) # Sort by person\n",
    "\n",
    "        index = int(df.shape[0] * (1 - ratio)) # Get the index of the last person to be in the training set\n",
    "        tempnum = df[\"Person\"].iloc[index] # Get the person number of the last person to be in the training set\n",
    "\n",
    "        temp = index\n",
    "        while temp < df.shape[0]:\n",
    "            temp += 1\n",
    "            if df[\"Person\"].iloc[temp] != tempnum:\n",
    "                index = temp - 1\n",
    "                break\n",
    "\n",
    "        print(f\"Persons 0 to {tempnum} are in the training set, and {tempnum + 1} to {df['Person'].iloc[-1]} are in the testing set\")\n",
    "        \n",
    "        return filterColumn(df.iloc[:index]), filterColumn(df.iloc[index:])\n",
    "    else:\n",
    "        \n",
    "        Test = df.loc[~df['Person'].isin(lst)]\n",
    "        Train = df.loc[df['Person'].isin(lst)]\n",
    "\n",
    "        return filterColumn(Train), filterColumn(Test)\n",
    "        \n",
    "def displayHeatmap(df):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "\n",
    "def displayConfusion(actual, predicted):\n",
    "    sk.ConfusionMatrixDisplay(sk.confusion_matrix(actual, predicted)).plot()\n",
    "    print(\"Accuracy is \", round(sk.accuracy_score(actual, predicted) * 100, 2), \"%\")\n",
    "\n",
    "def filterColumn(df, colList = featuresToKeep):\n",
    "    currdf = df\n",
    "    for col in currdf.columns:\n",
    "        if (str(col) not in colList):\n",
    "            currdf = currdf.drop(columns = [str(col)])\n",
    "\n",
    "    return currdf\n",
    "\n",
    "def filterConfidence(df, colList = featuresToKeep):\n",
    "    currdf = df\n",
    "    currdf = filterColumn(currdf, colList)\n",
    "\n",
    "    currdf = currdf.query(\"confidence >= 0.9\")\n",
    "    return currdf.drop(columns = [\"confidence\"]).dropna()\n",
    "\n",
    "def veticalMerge(df1, df2, shuffle = False):\n",
    "    df = pd.concat([df1, df2]).reset_index()\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1)\n",
    "    return df\n",
    "\n",
    "def addTFLabel(df, TrueOrFalse):\n",
    "    if TrueOrFalse:\n",
    "        df[\"Result\"] = 1\n",
    "    elif not TrueOrFalse:\n",
    "        df[\"Result\"] = 0\n",
    "\n",
    "def shuffleDF(df):\n",
    "    return df.sample(frac=1)\n",
    "\n",
    "def addGazeDelta(currCSV):\n",
    "  for j in range(10, currCSV.shape[0]):\n",
    "      if currCSV.iloc[[j - 10]][\"confidence\"].iloc[0] >= 0.8:\n",
    "        currCSV.at[j, 'dgaze_0_x'] = abs(currCSV.at[j - 10, 'gaze_0_x'] - currCSV.at[j, 'gaze_0_x'])\n",
    "        currCSV.at[j, 'dgaze_0_y'] = abs(currCSV.at[j - 10, 'gaze_0_y'] - currCSV.at[j, 'gaze_0_y'])\n",
    "        currCSV.at[j, 'dgaze_0_z'] = abs(currCSV.at[j - 10, 'gaze_0_z'] - currCSV.at[j, 'gaze_0_z'])\n",
    "        currCSV.at[j, 'dgaze_angle_x'] = abs(currCSV.at[j - 10, 'gaze_angle_x'] - currCSV.at[j, 'gaze_angle_x'])\n",
    "        currCSV.at[j, 'dgaze_angle_y'] = abs(currCSV.at[j - 10, 'gaze_angle_y'] - currCSV.at[j, 'gaze_angle_y'])\n",
    "\n",
    "  return currCSV\n",
    "\n",
    "def predictRF(df, modelName, modelObj):\n",
    "    \n",
    "    counterLie, counterTrue = 0, 0\n",
    "\n",
    "    if modelName == 'tf':\n",
    "        dataSet = tfdf.keras.pd_dataframe_to_tf_dataset(df)\n",
    "        res = pd.DataFrame(modelObj.predict(dataSet))\n",
    "\n",
    "        for i in range(res.shape[0]):\n",
    "            if res.iloc[i,0] > 0.5: \n",
    "                res.iloc[i] = 1 \n",
    "            else:\n",
    "                res.iloc[i] = 0\n",
    "\n",
    "    elif modelName == \"sk\":\n",
    "        res = modelObj.predict(df)\n",
    "        temp = res.shape[0]\n",
    "        res = pd.DataFrame(np.reshape(res, (temp, 1)))\n",
    "\n",
    "    for i in range(res.shape[0]):\n",
    "        if res.iloc[i][0] > 0.5:\n",
    "            counterTrue = counterTrue + 1\n",
    "        else:\n",
    "            counterLie = counterLie + 1\n",
    "            \n",
    "    print(\"Lie Possibility: \", round(counterLie/res.shape[0] * 100, 2), \"%\")\n",
    "    print(\"Truth Possibility: \", round(counterTrue/res.shape[0]* 100, 2), \"%\")\n",
    "\n",
    "def predictLSTM(df, modelObj):\n",
    "    return modelObj.predict(filterConfidence(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import tensorflow_decision_forests as tfdf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "truthPath = './processed_truth/'\n",
    "liePath = './processed_lie/'\n",
    "\n",
    "featuresToKeep = featuresToKeep\n",
    "\n",
    "newFeaturesToKeep = [\"gaze_0_x\",\"gaze_0_y\",\"gaze_0_z\",\"gaze_angle_x\", \"gaze_angle_y\", \"AU01_r\",\"AU04_r\",\"AU10_r\",\"AU12_r\",\"AU45_r\"]\n",
    "#[\"pose_Tx\",\"pose_Ty\", \"pose_Tz\", \"pose_Ry\"]\n",
    "\n",
    "# create a single dataset from a specified patha (must be all truth or all lie)\n",
    "def createDatasetSingle(path, truth):\n",
    "  df = pd.concat(map(pd.read_csv, glob.glob(os.path.join(path+\"*.csv\")))).reset_index()\n",
    "  addGazeDelta(df)\n",
    "  addTFLabel(df, truth)\n",
    "  df = filterColumn(df)\n",
    "\n",
    "  return df\n",
    "\n",
    "# input a truthpath and a liepath, create a dual dataset and create a train\n",
    "# test split based on the testRatio\n",
    "# outputs total train, train with x, train with y, test with x, and test with y\n",
    "def createDatasetRF(truthPath, liePath, testRatio, byPerson = False, personlst = []):\n",
    "  dfT = createDatasetSingle(truthPath, True)\n",
    "  dfL = createDatasetSingle(liePath, False)\n",
    "  \n",
    "  dfTotal = veticalMerge(dfT, dfL, shuffle=True)\n",
    "  \n",
    "  if byPerson:\n",
    "    Train, Test = shuffleByPerson(dfTotal, testRatio, personlst)\n",
    "  else:\n",
    "    Train, Test = train_test_split(dfTotal, test_size=testRatio, shuffle=False)\n",
    "\n",
    "  Xtrain, Ytrain = Train.reset_index().drop(columns = [\"Result\", \"Person\", \"index\", \"level_0\"]), Train[\"Result\"]\n",
    "  Xtest, Ytest = Test.reset_index().drop(columns = [\"Result\", \"Person\", \"index\", \"level_0\"]), Test[\"Result\"]\n",
    "  Train = Train.reset_index().drop(columns = [\"index\", \"Person\", \"level_0\"])\n",
    "\n",
    "  return Train, Xtrain, Ytrain, Xtest, Ytest\n",
    "\n",
    "def createDatasetLSTM(truthPath, liePath, testRatio, numFrames=10, minConfidence=0.9, byPerson=False, personlst = []):\n",
    "  dfT = createDatasetSingle(truthPath, True)\n",
    "  dfL = createDatasetSingle(liePath, False)\n",
    "\n",
    "  dfMap = {1:dfT, 0:dfL}\n",
    "\n",
    "  Xtrain, Ytrain, Xtest, Ytest = [], [], [], []\n",
    "\n",
    "  idxTotext = {0:\"Lie\", 1:\"Truth\"}\n",
    "\n",
    "  for idx in dfMap:\n",
    "    print(f'Processing {idxTotext[idx]}')\n",
    "    \n",
    "    if byPerson:\n",
    "      Train, Test = shuffleByPerson(dfMap[idx], lst = personlst)\n",
    "    elif not byPerson:\n",
    "      Train, Test = shuffleByPerson(dfMap[idx], ratio = testRatio)\n",
    "    \n",
    "    print(f'Processing Train')\n",
    "    trainGroups = Train.groupby(\"Person\")\n",
    "\n",
    "    for i in trainGroups.groups:\n",
    "      currData = trainGroups.get_group(i).sort_index()\n",
    "      bad_frames = np.where(currData[\"confidence\"] < minConfidence)[0]\n",
    "      # print(f'Processing Person {i}, shape of data is {currData.shape}')\n",
    "      \n",
    "      blocksLst = getLSTMBlocks(bad_frames.tolist(), currData.shape[0], blockSize=numFrames, start=0)\n",
    "\n",
    "      for i, j in tqdm(blocksLst):\n",
    "        Xtrain.append(currData.iloc[i:j+1].reset_index().drop(columns = [\"index\", \"confidence\", \"Result\", \"Person\"]).to_numpy())\n",
    "        Ytrain.append(idx)\n",
    "      \n",
    "    print(f'Processing Test')\n",
    "    testGroups = Test.groupby(\"Person\")\n",
    "    for i in testGroups.groups:\n",
    "      currData = testGroups.get_group(i).sort_index()\n",
    "      bad_frames = np.where(currData[\"confidence\"] < minConfidence)[0]\n",
    "      # print(f'Processing Person {i}, shape of data is {currData.shape}')\n",
    "      \n",
    "      blocksLst = getLSTMBlocks(bad_frames.tolist(), currData.shape[0], blockSize=numFrames, start=0)\n",
    "\n",
    "      for i, j in tqdm(blocksLst):\n",
    "        Xtest.append(currData.iloc[i:j+1].reset_index().drop(columns = [\"index\", \"confidence\", \"Result\", \"Person\"]).to_numpy())\n",
    "        Ytest.append(idx)\n",
    "\n",
    "  Xtrain = np.array(Xtrain)\n",
    "  Ytrain = np.array(Ytrain)\n",
    "  Xtest = np.array(Xtest)\n",
    "  Ytest = np.array(Ytest)\n",
    "  print(f'Processing Person {i}, currData is {currData.shape}')\n",
    "\n",
    "  random.seed(random.randint(1, 100))\n",
    "\n",
    "  # Create an array of indices, then shuffle it\n",
    "  indices = np.arange(len(Xtrain)).astype(int)\n",
    "  np.random.shuffle(indices)\n",
    "\n",
    "  # Same order of indices for both X and Y\n",
    "  Xtrain  = Xtrain[indices]\n",
    "  Ytrain = Ytrain[indices]\n",
    "\n",
    "  random.seed(random.randint(1, 100))\n",
    "\n",
    "  # Create an array of indices, then shuffle it\n",
    "  indices = np.arange(len(Xtest)).astype(int)\n",
    "  np.random.shuffle(indices)\n",
    "\n",
    "  # Same order of indices for both X and Y\n",
    "  Xtest  = Xtest[indices]\n",
    "  Ytest = Ytest[indices]\n",
    "\n",
    "  return Xtrain, Ytrain, Xtest, Ytest\n",
    "\n",
    "def preprocessing(truthPath, liePath, additionalPath=None, minConfidence = 0.9, numOfFrames = 10, byPerson = False):\n",
    "\n",
    "  data = []\n",
    "  label = []\n",
    "\n",
    "  if not byPerson:\n",
    "\n",
    "    for file in sorted(os.listdir(truthPath)):\n",
    "      if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(truthPath + file)\n",
    "        \n",
    "        bad_frame = set(np.where(df[\"confidence\"] < minConfidence)[0])\n",
    "        df = filterColumn(df, colList=newFeaturesToKeep)\n",
    "\n",
    "        index = numOfFrames\n",
    "        next_index = numOfFrames\n",
    "        \n",
    "        while index < len(df):\n",
    "          if index not in bad_frame and index >= next_index:\n",
    "            data.append((df.iloc[index-numOfFrames:index]).to_numpy())\n",
    "            label.append(1)\n",
    "          elif index in bad_frame:\n",
    "            next_index = index + numOfFrames\n",
    "          index += 1\n",
    "\n",
    "    for file in sorted(os.listdir(liePath)):\n",
    "      if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(liePath + file)\n",
    "\n",
    "        bad_frame = set(np.where(df[\"confidence\"] < minConfidence)[0])\n",
    "        df= filterColumn(df, colList=newFeaturesToKeep)\n",
    "\n",
    "        index = numOfFrames\n",
    "        next_index = numOfFrames\n",
    "        \n",
    "        while index < len(df):\n",
    "          if index not in bad_frame and index >= next_index:\n",
    "            data.append((df.iloc[index-numOfFrames:index]).to_numpy())\n",
    "            label.append(0)\n",
    "          elif index in bad_frame:\n",
    "            next_index = index + numOfFrames\n",
    "          index += 1\n",
    "\n",
    "    if additionalPath:\n",
    "      for file in sorted(os.listdir(additionalPath)):\n",
    "        if file.endswith(\".csv\"):\n",
    "          df = pd.read_csv(additionalPath + file)\n",
    "          \n",
    "          bad_frame = set(np.where(df[\"confidence\"] < minConfidence)[0])\n",
    "          df= filterColumn(df, colList=newFeaturesToKeep)\n",
    "\n",
    "          index = numOfFrames\n",
    "          next_index = numOfFrames\n",
    "          \n",
    "          while index < len(df):\n",
    "            if index not in bad_frame and index >= next_index:\n",
    "              data.append((df.iloc[index-numOfFrames:index]).to_numpy())\n",
    "              if file.endswith(\"T.csv\"):\n",
    "                label.append(1)\n",
    "              elif file.endswith(\"L.csv\"):\n",
    "                label.append(0)\n",
    "            elif index in bad_frame:\n",
    "              next_index = index + numOfFrames\n",
    "            index += 1\n",
    "\n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "    random.seed(random.randint(1, 100))\n",
    "\n",
    "    # Create an array of indices, then shuffle it\n",
    "    indices = np.arange(len(data)).astype(int)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Same order of indices for both X and Y\n",
    "    data  = data[indices]\n",
    "    label = label[indices]\n",
    "\n",
    "  return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "IfbJrWtdBO32",
    "outputId": "db080630-9ec2-479c-be94-d1cc68e59007"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/austinshi/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import dataset\n",
    "# import test_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "RvnCFbDsBO34"
   },
   "outputs": [],
   "source": [
    "#Models - LSTM and Transformer\n",
    "class classifierLSTM(nn.Module):\n",
    "  \n",
    "  def __init__(self, input_size, hidden_size, frame_count, device, dropout = 0.3, output_size = 2):\n",
    "\n",
    "    super().__init__()\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.fc = nn.Linear((hidden_size * frame_count), output_size)\n",
    "    self.device = device\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = x.unsqueeze(0)\n",
    "    pred, _ = self.lstm(x)\n",
    "    dropped = self.dropout(pred)\n",
    "    data = dropped.reshape((dropped.shape[0], -1))\n",
    "    #reshape to [1, num_frames * hidden_size]\n",
    "    data = self.fc(data)\n",
    "    data = nn.functional.softmax(data, dim = 1).to(self.device)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "HXo9uSM-BO36"
   },
   "outputs": [],
   "source": [
    "#data prep\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lie_trial_path = './processed_lie/' #60 entries\n",
    "truth_trial_path = './processed_truth/' #61 entries\n",
    "MU3D_path = './processed/' # 300 entries\n",
    "\n",
    "# no split by person\n",
    "numOfFrames = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "oLnPps3pLZ8J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Truth\n",
      "Persons 0 to 29 are in the training set, and 30 to 36 are in the testing set\n",
      "Processing Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 395/395 [00:00<00:00, 2559.81it/s]\n",
      "100%|███████████████████████████████████████| 563/563 [00:00<00:00, 2555.51it/s]\n",
      "100%|█████████████████████████████████████| 6559/6559 [00:02<00:00, 2605.10it/s]\n",
      "100%|█████████████████████████████████████| 8145/8145 [00:03<00:00, 2609.44it/s]\n",
      "100%|█████████████████████████████████████████| 35/35 [00:00<00:00, 2374.65it/s]\n",
      "100%|███████████████████████████████████████| 490/490 [00:00<00:00, 2611.48it/s]\n",
      "100%|███████████████████████████████████████| 152/152 [00:00<00:00, 2521.77it/s]\n",
      "100%|███████████████████████████████████████| 239/239 [00:00<00:00, 2572.32it/s]\n",
      "100%|███████████████████████████████████████| 802/802 [00:00<00:00, 2588.00it/s]\n",
      "100%|███████████████████████████████████████| 666/666 [00:00<00:00, 2644.61it/s]\n",
      "100%|█████████████████████████████████████| 1225/1225 [00:00<00:00, 2577.13it/s]\n",
      "100%|███████████████████████████████████████| 721/721 [00:00<00:00, 2593.79it/s]\n",
      "100%|███████████████████████████████████████| 394/394 [00:00<00:00, 2579.89it/s]\n",
      "100%|█████████████████████████████████████████| 12/12 [00:00<00:00, 2343.73it/s]\n",
      "100%|█████████████████████████████████████████| 33/33 [00:00<00:00, 2300.92it/s]\n",
      "100%|███████████████████████████████████████| 746/746 [00:00<00:00, 2547.84it/s]\n",
      "100%|███████████████████████████████████████| 147/147 [00:00<00:00, 2518.97it/s]\n",
      "100%|███████████████████████████████████████| 900/900 [00:00<00:00, 2549.99it/s]\n",
      "100%|███████████████████████████████████████| 119/119 [00:00<00:00, 2615.89it/s]\n",
      "100%|█████████████████████████████████████| 1644/1644 [00:00<00:00, 2618.83it/s]\n",
      "100%|███████████████████████████████████████| 470/470 [00:00<00:00, 2586.79it/s]\n",
      "100%|███████████████████████████████████████| 442/442 [00:00<00:00, 2556.43it/s]\n",
      "100%|███████████████████████████████████████| 520/520 [00:00<00:00, 2603.93it/s]\n",
      "100%|███████████████████████████████████████| 765/765 [00:00<00:00, 2625.12it/s]\n",
      "100%|███████████████████████████████████████| 334/334 [00:00<00:00, 2611.09it/s]\n",
      "100%|███████████████████████████████████████| 171/171 [00:00<00:00, 2590.32it/s]\n",
      "100%|███████████████████████████████████████| 260/260 [00:00<00:00, 2401.03it/s]\n",
      "100%|███████████████████████████████████████| 155/155 [00:00<00:00, 2500.69it/s]\n",
      "100%|███████████████████████████████████████| 119/119 [00:00<00:00, 2564.43it/s]\n",
      "100%|███████████████████████████████████████| 202/202 [00:00<00:00, 2539.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|███████████████████████████████████████| 221/221 [00:00<00:00, 2608.07it/s]\n",
      "100%|███████████████████████████████████████| 186/186 [00:00<00:00, 2606.61it/s]\n",
      "100%|███████████████████████████████████████| 848/848 [00:00<00:00, 2549.17it/s]\n",
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 2077.21it/s]\n",
      "100%|█████████████████████████████████████| 1056/1056 [00:00<00:00, 2542.58it/s]\n",
      "100%|█████████████████████████████████████| 2600/2600 [00:01<00:00, 2564.32it/s]\n",
      "100%|█████████████████████████████████████| 2428/2428 [00:00<00:00, 2565.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Lie\n",
      "Persons 0 to 46 are in the training set, and 47 to 58 are in the testing set\n",
      "Processing Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4393/4393 [00:01<00:00, 2615.91it/s]\n",
      "100%|█████████████████████████████████████| 4428/4428 [00:01<00:00, 2610.77it/s]\n",
      "100%|█████████████████████████████████████| 2617/2617 [00:01<00:00, 2547.29it/s]\n",
      "100%|█████████████████████████████████████| 4462/4462 [00:01<00:00, 2563.81it/s]\n",
      "100%|███████████████████████████████████████| 690/690 [00:00<00:00, 2547.85it/s]\n",
      "100%|███████████████████████████████████████| 429/429 [00:00<00:00, 2560.58it/s]\n",
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 2540.29it/s]\n",
      "100%|███████████████████████████████████████| 168/168 [00:00<00:00, 2526.36it/s]\n",
      "100%|███████████████████████████████████████| 263/263 [00:00<00:00, 2516.10it/s]\n",
      "100%|███████████████████████████████████████| 503/503 [00:00<00:00, 2524.32it/s]\n",
      "100%|███████████████████████████████████████| 495/495 [00:00<00:00, 2569.22it/s]\n",
      "100%|███████████████████████████████████████| 301/301 [00:00<00:00, 2551.98it/s]\n",
      "100%|███████████████████████████████████████| 116/116 [00:00<00:00, 2415.10it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1192.24it/s]\n",
      "100%|█████████████████████████████████████| 3214/3214 [00:01<00:00, 2568.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|█████████████████████████████████████████| 70/70 [00:00<00:00, 2526.99it/s]\n",
      "100%|█████████████████████████████████████████| 75/75 [00:00<00:00, 2389.14it/s]\n",
      "100%|█████████████████████████████████████| 1148/1148 [00:00<00:00, 2552.40it/s]\n",
      "100%|█████████████████████████████████████████| 29/29 [00:00<00:00, 2339.04it/s]\n",
      "100%|███████████████████████████████████████| 339/339 [00:00<00:00, 2488.68it/s]\n",
      "100%|███████████████████████████████████████| 447/447 [00:00<00:00, 2576.92it/s]\n",
      "100%|███████████████████████████████████████| 576/576 [00:00<00:00, 2545.66it/s]\n",
      "100%|███████████████████████████████████████| 476/476 [00:00<00:00, 2546.20it/s]\n",
      "100%|███████████████████████████████████████| 569/569 [00:00<00:00, 2531.91it/s]\n",
      "100%|█████████████████████████████████████████| 81/81 [00:00<00:00, 2443.51it/s]\n",
      "100%|███████████████████████████████████████| 199/199 [00:00<00:00, 2459.31it/s]\n",
      "100%|███████████████████████████████████████| 246/246 [00:00<00:00, 2534.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Person 827, currData is (870, 17)\n"
     ]
    }
   ],
   "source": [
    "TEST_RATIO = 0.2\n",
    "\n",
    "xTrain, yTrain, xTest, yTest = createDatasetLSTM(truth_trial_path, lie_trial_path, TEST_RATIO, numFrames=numOfFrames)\n",
    "\n",
    "yTrain_temp, yTest_temp = [], []\n",
    "\n",
    "for i in range(yTrain.shape[0]):\n",
    "    yTrain_temp.append([1,0]) if yTrain[i] == 0 else yTrain_temp.append([0,1])\n",
    "\n",
    "for i in range(yTest.shape[0]):\n",
    "    yTest_temp.append([1,0]) if yTest[i] == 0 else yTest_temp.append([0,1])\n",
    "\n",
    "y_Train = torch.tensor(yTrain_temp, dtype=torch.float32).to(device)\n",
    "y_Test = torch.tensor(yTest_temp, dtype=torch.float32).to(device)\n",
    "\n",
    "x_Train = torch.tensor(xTrain, dtype=torch.float32).to(device)\n",
    "x_Test = torch.tensor(xTest, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50182, 10, 14])\n"
     ]
    }
   ],
   "source": [
    "print(x_Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "mJ0fNSMADvms",
    "outputId": "743ee446-dddd-4e2a-c92e-5906f25db492"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#model prep\n",
    "featCount = 14\n",
    "num_frames = 10\n",
    "encoder_layers = 2\n",
    "LSTM_hidden = 256\n",
    "\n",
    "LSTM = classifierLSTM(featCount, LSTM_hidden, num_frames, device)\n",
    "\n",
    "# training\n",
    "def train(model, xTrain, yTrain, xTest, yTest, epochs = 100, lr = 0.005, batch_size = 10):\n",
    "    \"\"\" Train a model on a dataset \"\"\"\n",
    "    \n",
    "    # create a data loader to handle batching\n",
    "    xTrain_loader =  torch.utils.data.DataLoader(xTrain, batch_size=batch_size, shuffle=False)\n",
    "    xTest_loader = torch.utils.data.DataLoader(xTest, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # create a loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # train\n",
    "\n",
    "        idx = 0\n",
    "        model.train()\n",
    "\n",
    "        tot_loss = 0\n",
    "        tot_acc = 0\n",
    "        for batch in xTrain_loader:\n",
    "\n",
    "            # get data\n",
    "            x_train = batch.to(device).float()\n",
    "            y_train = torch.tensor(yTrain[idx:min(idx+batch_size,len(yTrain))]).float().clone().detach().to(device)\n",
    "            \n",
    "            if x_train.shape == torch.Size([10, 10, 14]):\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                y_pred = model(x_train)\n",
    "\n",
    "                actual_batch = torch.argmax(y_train, dim=1).long()\n",
    "                my_pred_batch = torch.argmax(y_pred, dim=1).long()\n",
    "                tot_acc += ((actual_batch == my_pred_batch).sum().item() / len(actual_batch))\n",
    "                #print(\"actual for batch \", idx, \" is \", torch.argmax(y_train, dim=1).long())\n",
    "                #print(\"my prediction for batch \", idx, \" is \", torch.argmax(y_pred, dim=1).long())\n",
    "\n",
    "                # compute loss\n",
    "                loss = loss_fn(y_pred,torch.argmax(y_train, dim=1).long())\n",
    "\n",
    "                tot_loss += loss.item()\n",
    "\n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "\n",
    "                idx += batch_size\n",
    "            \n",
    "        total_loss = tot_loss / len(xTrain_loader)\n",
    "        total_acc = tot_acc / len(xTrain_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {total_acc:.4f}')\n",
    "\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "\n",
    "            with torch.no_grad():\n",
    "            \n",
    "                idx_test = 0\n",
    "                test_acc = 0\n",
    "                for batch in xTest_loader:\n",
    "                    xTest = batch.to(device).float()\n",
    "                    y_test = torch.tensor(yTest[idx_test:min(idx_test+batch_size,len(yTest))]).float().clone().detach().to(device)\n",
    "                    y_pred = model(xTest)\n",
    "\n",
    "                    actual_batch = torch.argmax(y_test, dim=1).long()\n",
    "                    my_pred_batch = torch.argmax(y_pred, dim=1).long()\n",
    "\n",
    "                    #compute test accuracy\n",
    "                    test_acc += (actual_batch == my_pred_batch).float().mean().item()\n",
    "                    idx_test += batch_size\n",
    "\n",
    "                test_acc /= len(xTest_loader)\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Test Accuracy: {test_acc:.4f}')  \n",
    "\n",
    "train(LSTM, x_Train, y_Train, x_Test, y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
