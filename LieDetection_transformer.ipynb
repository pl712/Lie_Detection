{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:28:52.216564: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Different layers\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Input, Dense\n",
    "from tensorflow.keras.layers import LayerNormalization, Layer\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, GlobalAveragePooling1D\n",
    "# For miscellaneous functions\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow import convert_to_tensor, string, float32, shape, range, reshape\n",
    "from tensorflow.keras import utils\n",
    "# Keras models\n",
    "from tensorflow.keras import Model, Sequential\n",
    "# For datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# For evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# For math/arrays\n",
    "import numpy as np\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sequences:  11314\n",
      "Total test sequences:  7532\n",
      "Target categories are:  ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset while removing headers, footers and quotes\n",
    "train_dataset = fetch_20newsgroups(subset='train', random_state=0,\n",
    "remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "train_X, train_Y = (train_dataset.data, train_dataset.target)\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = fetch_20newsgroups(subset='test', random_state=0,\n",
    "remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "test_X, test_Y = (test_dataset.data, test_dataset.target)\n",
    "# Target classes\n",
    "newsgroup_names = train_dataset.target_names\n",
    "# Total classes\n",
    "n_classes = len(train_dataset.target_names)\n",
    "# Convert to binary vectors to represent categories\n",
    "train_Y_categorical = utils.to_categorical(train_Y)\n",
    "test_Y_categorical = utils.to_categorical(test_Y)\n",
    "\n",
    "#Print statistics\n",
    "print(\"Total training sequences: \", len(train_X))\n",
    "print(\"Total test sequences: \", len(test_X))\n",
    "print(\"Target categories are: \", newsgroup_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary:  ['', '[UNK]', 'today', 'weather', 'is', 'i', 'happy', 'awesome', 'am']\n",
      "Vectorized words:  tf.Tensor(\n",
      "[[5 8 6 2 0 0 0 0]\n",
      " [2 3 4 7 0 0 0 0]], shape=(2, 8), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:28:58.358100: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "toy_sentences = [[\"I am happy today\"], [\"today weather is awesome\"]]\n",
    "\n",
    "# Create the TextVectorization layer\n",
    "toy_vectorize_layer = TextVectorization(\n",
    "output_sequence_length=8,\n",
    "max_tokens=15)\n",
    "\n",
    "# Learn a dictionary\n",
    "toy_vectorize_layer.adapt(Dataset.from_tensor_slices(toy_sentences))\n",
    "\n",
    "# Use the trained TextVectorization to replace each word by its\n",
    "# dictionary index\n",
    "toy_vectorized_words = toy_vectorize_layer(convert_to_tensor(toy_sentences, dtype=string))\n",
    "print(\"Dictionary: \", toy_vectorize_layer.get_vocabulary())\n",
    "print(\"Vectorized words: \", toy_vectorized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total distinct words to use\n",
    "vocab_size = 25000\n",
    "# Specify the maximum characters to consider in each newsgroup\n",
    "sequence_length = 300\n",
    "\n",
    "train_X_tensor = Dataset.from_tensor_slices(train_X)\n",
    "\n",
    "# TextVectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "output_sequence_length=sequence_length,\n",
    "max_tokens=vocab_size)\n",
    "\n",
    "# Adapt method trains the TextVectorization layer and\n",
    "# creates a dictionary\n",
    "vectorize_layer.adapt(train_X_tensor)\n",
    "\n",
    "# Convert all newsgroups in train_X to vectorized tensors\n",
    "train_X_tensors = convert_to_tensor(train_X, dtype=string)\n",
    "train_X_vectorized = vectorize_layer(train_X_tensors)\n",
    "\n",
    "# Convert all newsgroups in test_X to vectorized tensors\n",
    "test_X_tensors = convert_to_tensor(test_X, dtype=string)\n",
    "test_X_vectorized = vectorize_layer(test_X_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding for words\n",
    "toy_word_embedding_layer = Embedding(input_dim=15, output_dim=4)\n",
    "toy_embedded_words = toy_word_embedding_layer(toy_vectorized_words)\n",
    "\n",
    "# Embedding for positions\n",
    "toy_position_embedding_layer = Embedding(input_dim=8, output_dim=4)\n",
    "toy_positions = range(start=0, limit=8, delta=1)\n",
    "toy_embedded_positions = toy_position_embedding_layer(toy_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.word_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.position_embedding = Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
    "\n",
    "def call(self, tokens):\n",
    "    sequence_length = shape(tokens)[-1]\n",
    "    all_positions = range(start=0, limit=sequence_length, delta=1)\n",
    "    positions_encoding = self.position_embedding(all_positions)\n",
    "    words_encoding = self.word_embedding(tokens)\n",
    "    return positions_encoding + words_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multihead layer output: \n",
      " tf.Tensor([[[ 1.8797683  1.506011  -2.363825 ]]], shape=(1, 1, 3), dtype=float32)\n",
      "\n",
      "Multihead attention wts: \n",
      " tf.Tensor([[[[1.]]]], shape=(1, 1, 1, 1), dtype=float32)\n",
      "\n",
      "Total Layer weights:  8\n"
     ]
    }
   ],
   "source": [
    "toy_multihead = MultiHeadAttention(num_heads=1, key_dim=3)\n",
    "toy_x = np.array([[[1, 2, 3]]])\n",
    "toy_x_tensor = convert_to_tensor(toy_x, dtype=float32)\n",
    "toy_attn_output, toy_attn_wts = toy_multihead(toy_x_tensor, toy_x_tensor, return_attention_scores=True)\n",
    "\n",
    "print('Multihead layer output: \\n', toy_attn_output)\n",
    "print('\\nMultihead attention wts: \\n', toy_attn_wts)\n",
    "print('\\nTotal Layer weights: ', len(toy_multihead.get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, total_heads, total_dense_units, embed_dim):\n",
    "        super(EncoderLayer, self).__init__()# Multihead attention layer\n",
    "        self.multihead = MultiHeadAttention(num_heads=total_heads, key_dim=embed_dim)# Feed forward network layer\n",
    "        self.nnw = Sequential([Dense(total_dense_units, activation=\"relu\"),\n",
    "        Dense(embed_dim)])# Normalization\n",
    "        self.normalize_layer = LayerNormalization()\n",
    "\n",
    "def call(self, inputs):\n",
    "    attn_output = self.multihead(inputs, inputs)\n",
    "    normalize_attn = self.normalize_layer(inputs + attn_output)\n",
    "    nnw_output = self.nnw(normalize_attn)\n",
    "    final_output = self.normalize_layer(normalize_attn + nnw_output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " embedding_layer (EmbeddingL  (None, 300)              0         \n",
      " ayer)                                                           \n",
      "                                                                 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/LieDetection_transformer.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m transformer_model \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39moutputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m transformer_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPrecision\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRecall\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m transformer_model\u001b[39m.\u001b[39;49msummary()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:3219\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m   3214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3215\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis model has not yet been built. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3216\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3217\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe model on a batch of data.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3218\u001b[0m     )\n\u001b[0;32m-> 3219\u001b[0m layer_utils\u001b[39m.\u001b[39;49mprint_summary(\n\u001b[1;32m   3220\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   3221\u001b[0m     line_length\u001b[39m=\u001b[39;49mline_length,\n\u001b[1;32m   3222\u001b[0m     positions\u001b[39m=\u001b[39;49mpositions,\n\u001b[1;32m   3223\u001b[0m     print_fn\u001b[39m=\u001b[39;49mprint_fn,\n\u001b[1;32m   3224\u001b[0m     expand_nested\u001b[39m=\u001b[39;49mexpand_nested,\n\u001b[1;32m   3225\u001b[0m     show_trainable\u001b[39m=\u001b[39;49mshow_trainable,\n\u001b[1;32m   3226\u001b[0m     layer_range\u001b[39m=\u001b[39;49mlayer_range,\n\u001b[1;32m   3227\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/layer_utils.py:433\u001b[0m, in \u001b[0;36mprint_summary\u001b[0;34m(model, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m    426\u001b[0m         print_fn(\n\u001b[1;32m    427\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m nested_level\n\u001b[1;32m    428\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m (line_length \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m nested_level)\n\u001b[1;32m    429\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m nested_level\n\u001b[1;32m    430\u001b[0m         )\n\u001b[1;32m    432\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers[layer_range[\u001b[39m0\u001b[39m] : layer_range[\u001b[39m1\u001b[39m]]:\n\u001b[0;32m--> 433\u001b[0m     print_layer(layer)\n\u001b[1;32m    434\u001b[0m print_fn(\u001b[39m\"\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m line_length)\n\u001b[1;32m    436\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39m_collected_trainable_weights\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/layer_utils.py:401\u001b[0m, in \u001b[0;36mprint_summary.<locals>.print_layer\u001b[0;34m(layer, nested_level, is_nested_last)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_layer\u001b[39m(layer, nested_level\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, is_nested_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    400\u001b[0m     \u001b[39mif\u001b[39;00m sequential_like:\n\u001b[0;32m--> 401\u001b[0m         print_layer_summary(layer, nested_level)\n\u001b[1;32m    402\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         print_layer_summary_with_connections(layer, nested_level)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/layer_utils.py:347\u001b[0m, in \u001b[0;36mprint_summary.<locals>.print_layer_summary\u001b[0;34m(layer, nested_level)\u001b[0m\n\u001b[1;32m    345\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0 (unused)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     params \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mcount_params()\n\u001b[1;32m    348\u001b[0m fields \u001b[39m=\u001b[39m [name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m (\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m cls_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m, output_shape, params]\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m show_trainable:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:2113\u001b[0m, in \u001b[0;36mLayer.count_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2105\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2107\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou tried to call `count_params` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2108\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mon layer \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2111\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.build(batch_input_shape)`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2112\u001b[0m         )\n\u001b[0;32m-> 2113\u001b[0m \u001b[39mreturn\u001b[39;00m layer_utils\u001b[39m.\u001b[39mcount_params(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:1301\u001b[0m, in \u001b[0;36mLayer.weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   1295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1296\u001b[0m     \u001b[39m\"\"\"Returns the list of all layer variables/weights.\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \n\u001b[1;32m   1298\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[39m      A list of variables.\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_weights \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnon_trainable_weights\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:1259\u001b[0m, in \u001b[0;36mLayer.trainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[39m\"\"\"List of all trainable weights tracked by this layer.\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \n\u001b[1;32m   1253\u001b[0m \u001b[39mTrainable weights are updated via gradient descent during training.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[39m  A list of trainable variables.\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable:\n\u001b[0;32m-> 1259\u001b[0m     children_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gather_children_attribute(\n\u001b[1;32m   1260\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainable_variables\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m   1261\u001b[0m     )\n\u001b[1;32m   1262\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dedup_weights(\n\u001b[1;32m   1263\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainable_weights \u001b[39m+\u001b[39m children_weights\n\u001b[1;32m   1264\u001b[0m     )\n\u001b[1;32m   1265\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:3164\u001b[0m, in \u001b[0;36mLayer._gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   3160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_self_tracked_trackables\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   3161\u001b[0m     nested_layers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flatten_modules(\n\u001b[1;32m   3162\u001b[0m         include_self\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, recursive\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3163\u001b[0m     )\n\u001b[0;32m-> 3164\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m   3165\u001b[0m         itertools\u001b[39m.\u001b[39;49mchain\u001b[39m.\u001b[39;49mfrom_iterable(\n\u001b[1;32m   3166\u001b[0m             \u001b[39mgetattr\u001b[39;49m(layer, attribute) \u001b[39mfor\u001b[39;49;00m layer \u001b[39min\u001b[39;49;00m nested_layers\n\u001b[1;32m   3167\u001b[0m         )\n\u001b[1;32m   3168\u001b[0m     )\n\u001b[1;32m   3169\u001b[0m \u001b[39mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:3166\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_self_tracked_trackables\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   3161\u001b[0m     nested_layers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flatten_modules(\n\u001b[1;32m   3162\u001b[0m         include_self\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, recursive\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3163\u001b[0m     )\n\u001b[1;32m   3164\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3165\u001b[0m         itertools\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[0;32m-> 3166\u001b[0m             \u001b[39mgetattr\u001b[39;49m(layer, attribute) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m nested_layers\n\u001b[1;32m   3167\u001b[0m         )\n\u001b[1;32m   3168\u001b[0m     )\n\u001b[1;32m   3169\u001b[0m \u001b[39mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:2221\u001b[0m, in \u001b[0;36mLayer.trainable_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2219\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_generate_docs\n\u001b[1;32m   2220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrainable_variables\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 2221\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_weights\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2599\u001b[0m, in \u001b[0;36mModel.trainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2597\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2598\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrainable_weights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 2599\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assert_weights_created()\n\u001b[1;32m   2600\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainable:\n\u001b[1;32m   2601\u001b[0m         \u001b[39mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/sequential.py:511\u001b[0m, in \u001b[0;36mSequential._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39m# When the graph has not been initialized, use the Model's\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[39m# implementation to to check if the weights has been created.\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m \u001b[39msuper\u001b[39;49m(functional\u001b[39m.\u001b[39;49mFunctional, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_assert_weights_created()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:3467\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3456\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   3458\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3459\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbuild\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m   3460\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39m!=\u001b[39m Model\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3465\u001b[0m     \u001b[39m# Also make sure to exclude Model class itself which has build()\u001b[39;00m\n\u001b[1;32m   3466\u001b[0m     \u001b[39m# defined.\u001b[39;00m\n\u001b[0;32m-> 3467\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3468\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeights for model \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m have not yet been \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3469\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3470\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWeights are created when the Model is first called on \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3471\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3472\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "embed_dim = 64\n",
    "num_heads = 2\n",
    "total_dense_units = 60\n",
    "# Our two custom layers\n",
    "embedding_layer = EmbeddingLayer(sequence_length, vocab_size, embed_dim)\n",
    "encoder_layer = EncoderLayer(num_heads, total_dense_units, embed_dim)\n",
    "\n",
    "# Start connecting the layers together\n",
    "inputs = Input(shape=(sequence_length, ))\n",
    "emb = embedding_layer(inputs)\n",
    "enc = encoder_layer(emb)\n",
    "pool = GlobalAveragePooling1D()(enc)\n",
    "d = Dense(total_dense_units, activation=\"relu\")(pool)\n",
    "outputs = Dense(n_classes, activation=\"softmax\")(d)\n",
    "\n",
    "# Construct the transformer model\n",
    "transformer_model = Model(inputs=inputs, outputs=outputs)\n",
    "transformer_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy', 'Precision', 'Recall'])\n",
    "transformer_model.summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
