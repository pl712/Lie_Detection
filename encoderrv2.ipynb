{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Features = 14\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import imageio_ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.inception import Inception3\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "import imageio\n",
    "\n",
    "model_urls = {\n",
    "    # Inception v3 ported from TensorFlow\n",
    "    'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n",
    "}\n",
    "\n",
    "def inception_v3_sliced(pretrained=False, progress=True, stop_layer=3, **kwargs):\n",
    "    \"\"\"Inception v3 model architecture from\n",
    "    `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_.\n",
    "    .. note::\n",
    "        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n",
    "        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        aux_logits (bool): If True, add an auxiliary branch that can improve training.\n",
    "            Default: *True*\n",
    "        transform_input (bool): If True, preprocesses the input according to the method with which it\n",
    "            was trained on ImageNet. Default: *False*\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        if 'transform_input' not in kwargs:\n",
    "            kwargs['transform_input'] = True\n",
    "        if 'aux_logits' in kwargs:\n",
    "            original_aux_logits = kwargs['aux_logits']\n",
    "            kwargs['aux_logits'] = True\n",
    "        else:\n",
    "            original_aux_logits = True\n",
    "        kwargs['init_weights'] = False  # we are loading weights from a pretrained model\n",
    "        \n",
    "        class Inception3Mod(Inception3):\n",
    "\n",
    "            def __init__(self, stop_layer, **kwargs):\n",
    "                super(Inception3Mod, self).__init__(**kwargs)\n",
    "                self.stop_layer = stop_layer\n",
    "                \n",
    "            def _forward(self, x):\n",
    "                layers = [\n",
    "                self.Conv2d_1a_3x3,\n",
    "                self.Conv2d_2a_3x3,\n",
    "                self.Conv2d_2b_3x3,\n",
    "                'maxpool',\n",
    "                self.Conv2d_3b_1x1,\n",
    "                self.Conv2d_4a_3x3,\n",
    "                'maxpool',\n",
    "                self.Mixed_5b,\n",
    "                self.Mixed_5c,\n",
    "                self.Mixed_5d,\n",
    "                self.Mixed_6a,\n",
    "                self.Mixed_6b,\n",
    "                self.Mixed_6c,\n",
    "                self.Mixed_6d,\n",
    "                self.Mixed_6e,\n",
    "                self.Mixed_7a,\n",
    "                self.Mixed_7b,\n",
    "                self.Mixed_7c,\n",
    "                ]\n",
    "\n",
    "                for idx in range(self.stop_layer):\n",
    "                    layer = layers[idx]\n",
    "                    if layer == 'maxpool':\n",
    "                      x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "                    else:\n",
    "                      x = layer(x)\n",
    "                return x, None\n",
    "\n",
    "\n",
    "        model = Inception3Mod(**kwargs, stop_layer=stop_layer)\n",
    "        state_dict = load_state_dict_from_url(model_urls['inception_v3_google'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "        if not original_aux_logits:\n",
    "            model.aux_logits = False\n",
    "            del model.AuxLogits\n",
    "        return model\n",
    "\n",
    "    return Inception3Mod(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e\n",
    "class positionalEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self, frame_length, encoding_length):\n",
    "    super().__init__()\n",
    "\n",
    "    embedding = nn.Embedding(frame_length, encoding_length)\n",
    "\n",
    "    self.pe = embedding(torch.tensor([i for i in range(frame_length)]).unsqueeze(1)).squeeze()\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    embedded = []\n",
    "\n",
    "    if len(x.shape) == 3:\n",
    "      for i in range(x.shape[0]):\n",
    "        embedded.append(torch.cat((x[i], self.pe), 1).detach().numpy())\n",
    "      return torch.tensor(embedded)\n",
    "    else:\n",
    "      return torch.cat((x, self.pe[0:x.shape[0]]), 1)\n",
    "\n",
    "class encoderTransformer(nn.Module):\n",
    "\n",
    "  def __init__(self, num_T_layers, num_fc_layers, outFeatCount, device, num_frames_max = 1000, stopLayer = 16, testLayer = None, train = True, pos_encode_size = 3, n_hidden = 2048, n_heads = 86, dropout = 0.3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.posEncoder = positionalEncoder(num_frames_max, 10)\n",
    "    \n",
    "    self.visionEncoder = inception_v3_sliced(pretrained=True, \n",
    "                                             transform_input = True,\n",
    "                                             stop_layer=stopLayer)\n",
    "    \n",
    "    self.transform = Compose([ToPILImage(),Resize((299, 299)), ToTensor()])\n",
    "\n",
    "    inFeatCount = 1280 + 10\n",
    "    \n",
    "    encoder_layer = nn.TransformerEncoderLayer(inFeatCount, n_heads, n_hidden, dropout)\n",
    "    self.encoder = nn.TransformerEncoder(encoder_layer, num_T_layers)\n",
    "\n",
    "    self.fcLayers = []\n",
    "    currInput = inFeatCount\n",
    "\n",
    "    for i in range(num_fc_layers):\n",
    "      if i == num_fc_layers - 1:\n",
    "        self.fcLayers.append(nn.Linear(currInput, outFeatCount))\n",
    "      else:\n",
    "        self.fcLayers.append(nn.Linear(currInput, currInput // 2))\n",
    "        currInput = currInput // 2\n",
    "\n",
    "    self.device = device\n",
    "    self.train = train\n",
    "    \n",
    "    self.outputLayer = testLayer if (testLayer and testLayer < len(self.fcLayers) - 1) else len(self.fcLayers) - 1\n",
    "    \n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "\n",
    "      for i in self.fcLayers:\n",
    "        i.bias.data.zero_()\n",
    "        i.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, vidLink):\n",
    "\n",
    "    vid = imageio.get_reader(vidLink)\n",
    "\n",
    "    imgArr = []\n",
    "\n",
    "    frameCt = 0\n",
    "\n",
    "    for frame_number, im in enumerate(vid):\n",
    "      imgArr.append(im) \n",
    "\n",
    "    Arr = []\n",
    "\n",
    "    for i in imgArr:\n",
    "      img = self.transform(i)[None]\n",
    "\n",
    "      picToVal = self.visionEncoder(img)[0]\n",
    "      to1D = nn.MaxPool2d((8, 8))(picToVal).squeeze()\n",
    "      Arr.append(to1D.tolist())\n",
    "\n",
    "    encoded = self.posEncoder(torch.Tensor(Arr))\n",
    "    \n",
    "    result = self.encoder(encoded)\n",
    "\n",
    "    if self.train:\n",
    "      for i in self.fcLayers:\n",
    "        result = i(result)\n",
    "    else:\n",
    "      for i in range(self.outputLayer):\n",
    "        result = self.fcLayers[i](result)\n",
    "\n",
    "    os.system(\"rm ./*.jpg\")\n",
    "\n",
    "    return result\n",
    "\n",
    "class classifierTransformer(nn.Module):\n",
    "\n",
    "  def __init__(self, inFeatCount, num_T_layers, num_fc_layers, num_frames, device, pos_encode_size = 5, n_heads = 4, n_hidden = 2048, dropout = 0.3, outFeatCount = 2):\n",
    "    super().__init__()\n",
    "\n",
    "    self.posEncoder = positionalEncoder(num_frames, pos_encode_size)\n",
    "\n",
    "    num_features = inFeatCount + pos_encode_size\n",
    "\n",
    "    n_hidden = max(n_hidden, 2*num_features)\n",
    "\n",
    "    encoder_layer = nn.TransformerEncoderLayer(inFeatCount + pos_encode_size, n_heads, n_hidden, dropout)\n",
    "    self.encoder = nn.TransformerEncoder(encoder_layer, num_T_layers)\n",
    "    \n",
    "    currInput = num_frames * num_features\n",
    "\n",
    "    self.fcLayers = []\n",
    "\n",
    "    for i in range(num_fc_layers):\n",
    "      if i == num_fc_layers - 1:\n",
    "        self.fcLayers.append(nn.Linear(currInput, outFeatCount))\n",
    "      else:\n",
    "        self.fcLayers.append(nn.Linear(currInput, currInput // 2))\n",
    "        currInput = currInput // 2\n",
    "\n",
    "    self.device = device\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "\n",
    "      for i in self.fcLayers:\n",
    "        i.bias.data.zero_()\n",
    "        i.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #x.shape = [num_frames, feat_count]\n",
    "    encoded = self.posEncoder(x)\n",
    "    #encoded.shape = [num_frames, feat_count + pos_encoding_count]\n",
    "    data = self.encoder(encoded)\n",
    "    #data.shape = [num_frames, feat_count + pos_encoding_count]\n",
    "    data = torch.reshape(data, (1,-1))\n",
    "    #data.shape = [1, num_frames * (feat_count + pos_encoding_count)] \n",
    "\n",
    "    for i in self.fcLayers:\n",
    "        data = i(data)     \n",
    "\n",
    "    return torch.tensor(data.tolist()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"User_0_run_0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: ./*.jpg: No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([510, 15])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = encoderTransformer(1, 1, 15, 'cpu')\n",
    "\n",
    "res = model(\"trial_lie_001.mp4\")\n",
    "\n",
    "res.shape # -> (n_frames = 114, n_features = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([510, 15])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "openface = pd.read_csv(\"processed_lie/trial_lie_001.csv\")\n",
    "openface = openface[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x', 'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r'\\\n",
    ",'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "openface = torch.tensor(np.array(openface)).float()\n",
    "enoder_loss_fuction = nn.MSELoss()\n",
    "print(openface.shape)\n",
    "loss = enoder_loss_fuction(res, openface)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
