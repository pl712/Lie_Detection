{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Features = 14\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import imageio_ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.inception import Inception3\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "import imageio\n",
    "\n",
    "model_urls = {\n",
    "    # Inception v3 ported from TensorFlow\n",
    "    'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n",
    "}\n",
    "\n",
    "def inception_v3_sliced(pretrained=False, progress=True, stop_layer=3, **kwargs):\n",
    "    \"\"\"Inception v3 model architecture from\n",
    "    `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_.\n",
    "    .. note::\n",
    "        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n",
    "        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        aux_logits (bool): If True, add an auxiliary branch that can improve training.\n",
    "            Default: *True*\n",
    "        transform_input (bool): If True, preprocesses the input according to the method with which it\n",
    "            was trained on ImageNet. Default: *False*\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        if 'transform_input' not in kwargs:\n",
    "            kwargs['transform_input'] = True\n",
    "        if 'aux_logits' in kwargs:\n",
    "            original_aux_logits = kwargs['aux_logits']\n",
    "            kwargs['aux_logits'] = True\n",
    "        else:\n",
    "            original_aux_logits = True\n",
    "        kwargs['init_weights'] = False  # we are loading weights from a pretrained model\n",
    "        \n",
    "        class Inception3Mod(Inception3):\n",
    "\n",
    "            def __init__(self, stop_layer, **kwargs):\n",
    "                super(Inception3Mod, self).__init__(**kwargs)\n",
    "                self.stop_layer = stop_layer\n",
    "                \n",
    "            def _forward(self, x):\n",
    "                layers = [\n",
    "                self.Conv2d_1a_3x3,\n",
    "                self.Conv2d_2a_3x3,\n",
    "                self.Conv2d_2b_3x3,\n",
    "                'maxpool',\n",
    "                self.Conv2d_3b_1x1,\n",
    "                self.Conv2d_4a_3x3,\n",
    "                'maxpool',\n",
    "                self.Mixed_5b,\n",
    "                self.Mixed_5c,\n",
    "                self.Mixed_5d,\n",
    "                self.Mixed_6a,\n",
    "                self.Mixed_6b,\n",
    "                self.Mixed_6c,\n",
    "                self.Mixed_6d,\n",
    "                self.Mixed_6e,\n",
    "                self.Mixed_7a,\n",
    "                self.Mixed_7b,\n",
    "                self.Mixed_7c,\n",
    "                ]\n",
    "\n",
    "                for idx in range(self.stop_layer):\n",
    "                    layer = layers[idx]\n",
    "                    if layer == 'maxpool':\n",
    "                      x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "                    else:\n",
    "                      x = layer(x)\n",
    "                return x, None\n",
    "\n",
    "\n",
    "        model = Inception3Mod(**kwargs, stop_layer=stop_layer)\n",
    "        state_dict = load_state_dict_from_url(model_urls['inception_v3_google'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "        if not original_aux_logits:\n",
    "            model.aux_logits = False\n",
    "            del model.AuxLogits\n",
    "        return model\n",
    "\n",
    "    return Inception3Mod(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self, frame_length, encoding_length):\n",
    "    super().__init__()\n",
    "\n",
    "    embedding = nn.Embedding(frame_length, encoding_length)\n",
    "\n",
    "    self.pe = embedding(torch.tensor([i for i in range(frame_length)]).unsqueeze(1)).squeeze()\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    embedded = []\n",
    "\n",
    "    if len(x.shape) == 3:\n",
    "      for i in range(x.shape[0]):\n",
    "        embedded.append(torch.cat((x[i], self.pe), 1).detach().numpy())\n",
    "      return torch.tensor(embedded)\n",
    "    else:\n",
    "      return torch.cat((x, self.pe[0:x.shape[0]]), 1)\n",
    "\n",
    "class encoderTransformer(nn.Module):\n",
    "\n",
    "  def __init__(self, num_T_layers, num_fc_layers, outFeatCount, device, num_frames_max = 1000, stopLayer = 16, testLayer = None, train = True, pos_encode_size = 3, n_hidden = 2048, n_heads = 86, dropout = 0.3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.posEncoder = positionalEncoder(num_frames_max, 10)\n",
    "    \n",
    "    self.visionEncoder = inception_v3_sliced(pretrained=True, \n",
    "                                             transform_input = True,\n",
    "                                             stop_layer=stopLayer)\n",
    "    \n",
    "    self.transform = Compose([ToPILImage(),Resize((299, 299)), ToTensor()])\n",
    "\n",
    "    inFeatCount = 1280 + 10\n",
    "    \n",
    "    encoder_layer = nn.TransformerEncoderLayer(inFeatCount, n_heads, n_hidden, dropout)\n",
    "    self.encoder = nn.TransformerEncoder(encoder_layer, num_T_layers)\n",
    "\n",
    "    self.fcLayers = []\n",
    "    currInput = inFeatCount\n",
    "\n",
    "    for i in range(num_fc_layers):\n",
    "      if i == num_fc_layers - 1:\n",
    "        self.fcLayers.append(nn.Linear(currInput, outFeatCount))\n",
    "      else:\n",
    "        self.fcLayers.append(nn.Linear(currInput, currInput // 2))\n",
    "        currInput = currInput // 2\n",
    "\n",
    "    self.device = device\n",
    "    self.train = train\n",
    "    \n",
    "    self.outputLayer = testLayer if (testLayer and testLayer < len(self.fcLayers) - 1) else len(self.fcLayers) - 1\n",
    "    \n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "\n",
    "      for i in self.fcLayers:\n",
    "        i.bias.data.zero_()\n",
    "        i.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, imgArr):\n",
    "\n",
    "    # vid = imageio.get_reader(vidLink)\n",
    "\n",
    "    # imgArr = []\n",
    "\n",
    "    # frameCt = 0\n",
    "\n",
    "    # for frame_number, im in enumerate(vid):\n",
    "    #   imgArr.append(im) \n",
    "\n",
    "    # print(np.array(imgArr).shape)\n",
    "\n",
    "    Arr = []\n",
    "\n",
    "    for i in imgArr:\n",
    "      img = self.transform(i)[None]\n",
    "\n",
    "      picToVal = self.visionEncoder(img)[0]\n",
    "      to1D = nn.MaxPool2d((8, 8))(picToVal).squeeze()\n",
    "      Arr.append(to1D.tolist())\n",
    "\n",
    "    encoded = self.posEncoder(torch.Tensor(Arr))\n",
    "    \n",
    "    result = self.encoder(encoded)\n",
    "\n",
    "    if self.train:\n",
    "      for i in self.fcLayers:\n",
    "        result = i(result)\n",
    "    else:\n",
    "      for i in range(self.outputLayer):\n",
    "        result = self.fcLayers[i](result)\n",
    "\n",
    "    #os.system(\"rm ./*.jpg\")\n",
    "\n",
    "    return result\n",
    "\n",
    "class classifierTransformer(nn.Module):\n",
    "\n",
    "  def __init__(self, inFeatCount, num_T_layers, num_fc_layers, num_frames, device, pos_encode_size = 5, n_heads = 4, n_hidden = 2048, dropout = 0.3, outFeatCount = 2):\n",
    "    super().__init__()\n",
    "\n",
    "    self.posEncoder = positionalEncoder(num_frames, pos_encode_size)\n",
    "\n",
    "    num_features = inFeatCount + pos_encode_size\n",
    "\n",
    "    n_hidden = max(n_hidden, 2*num_features)\n",
    "\n",
    "    encoder_layer = nn.TransformerEncoderLayer(inFeatCount + pos_encode_size, n_heads, n_hidden, dropout)\n",
    "    self.encoder = nn.TransformerEncoder(encoder_layer, num_T_layers)\n",
    "    \n",
    "    currInput = num_frames * num_features\n",
    "\n",
    "    self.fcLayers = []\n",
    "\n",
    "    for i in range(num_fc_layers):\n",
    "      if i == num_fc_layers - 1:\n",
    "        self.fcLayers.append(nn.Linear(currInput, outFeatCount))\n",
    "      else:\n",
    "        self.fcLayers.append(nn.Linear(currInput, currInput // 2))\n",
    "        currInput = currInput // 2\n",
    "\n",
    "    self.device = device\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "\n",
    "      for i in self.fcLayers:\n",
    "        i.bias.data.zero_()\n",
    "        i.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #x.shape = [num_frames, feat_count]\n",
    "    encoded = self.posEncoder(x)\n",
    "    #encoded.shape = [num_frames, feat_count + pos_encoding_count]\n",
    "    data = self.encoder(encoded)\n",
    "    #data.shape = [num_frames, feat_count + pos_encoding_count]\n",
    "    data = torch.reshape(data, (1,-1))\n",
    "    #data.shape = [1, num_frames * (feat_count + pos_encoding_count)] \n",
    "\n",
    "    for i in self.fcLayers:\n",
    "        data = i(data)     \n",
    "\n",
    "    return torch.tensor(data.tolist()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = encoderTransformer(1, 1, 15, 'cpu')\n",
    "\n",
    "# res = model(\"trial_lie_001.mp4\")\n",
    "\n",
    "# res.shape # -> (n_frames = 114, n_features = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "openface = pd.read_csv(\"processed_lie/trial_lie_001.csv\")\n",
    "openface = openface[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x', 'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r'\\\n",
    ",'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "openface = torch.tensor(np.array(openface)).float()\n",
    "encoder_loss_fuction = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video:  trial_lie_041.mp4 finished:  0\n"
     ]
    }
   ],
   "source": [
    "# define a preprocessing loop\n",
    "def preprocess(path):\n",
    "    imgArr = []\n",
    "    Openface_Arr = pd.DataFrame()\n",
    "    idx = 0\n",
    "    for video in os.listdir(path):\n",
    "        if video == 'trial_lie_043.mp4':\n",
    "            continue\n",
    "        if idx == 10:\n",
    "            break\n",
    "        if video.endswith(\".mp4\"):\n",
    "            print('Processing video: ', video, 'finished: ', idx)\n",
    "            vid = imageio.get_reader(path + \"/\" + video)\n",
    "            for frame_number, im in enumerate(vid):\n",
    "                im = cv2.resize(im, (240, 320))\n",
    "                imgArr.append(im)\n",
    "            \n",
    "            if video[:-4] + \".csv\" in os.listdir(\"processed_lie\"):\n",
    "                file = pd.read_csv(\"processed_lie/\" + video[:-4] + \".csv\")\n",
    "                file = file[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x',\n",
    "                                'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r',\n",
    "                                'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "                Openface_Arr = pd.concat([Openface_Arr, file])\n",
    "            if video[:-4] + \".csv\" in os.listdir(\"processed_truth\"):\n",
    "                file = pd.read_csv(\"processed_truth/\" + video[:-4] + \".csv\")\n",
    "                file = file[['gaze_0_x','gaze_0_y','gaze_0_z','gaze_angle_x',\n",
    "                                'gaze_angle_y','AU01_r','AU02_r','AU04_r','AU05_r',\n",
    "                                'AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r']]\n",
    "                Openface_Arr = pd.concat([Openface_Arr, file])\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    return imgArr, np.array(Openface_Arr)\n",
    "\n",
    "imgArr, Openface_Arr = preprocess(\"videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = encoderTransformer(1, 1, 15, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test train split\n",
    "def train_test_split(imgArr, Openface_Arr, test_size, random_state):\n",
    "    np.random.seed(random_state)\n",
    "    idx = np.random.permutation(len(imgArr))\n",
    "    test_size = int(len(imgArr) * test_size)\n",
    "    X_train = [imgArr[i] for i in idx[test_size:]]\n",
    "    X_test = [imgArr[i] for i in idx[:test_size]]\n",
    "    y_train = [Openface_Arr[i] for i in idx[test_size:]]\n",
    "    y_test = [Openface_Arr[i] for i in idx[:test_size]]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgArr, Openface_Arr, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: ./*.jpg: No such file or directory\n",
      "rm: ./*.jpg: No such file or directory\n",
      "rm: ./*.jpg: No such file or directory\n",
      "rm: ./*.jpg: No such file or directory\n",
      "rm: ./*.jpg: No such file or directory\n",
      "rm: ./*.jpg: No such file or directory\n",
      "rm: ./*.jpg: No such file or directory\n",
      "rm: ./*.jpg: No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/encoderrv2.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m train_loss, val_loss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m train(training_model, X_train, y_train, X_test, y_test, \u001b[39m20\u001b[39;49m, \u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/encoderrv2.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, xtrain, ytrain, xtest, ytest, epochs, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m x \u001b[39m=\u001b[39m xtrain[i:\u001b[39mmin\u001b[39m(i\u001b[39m+\u001b[39mbatch_size, \u001b[39mlen\u001b[39m(xtrain))]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(ytrain[i:\u001b[39mmin\u001b[39m(i\u001b[39m+\u001b[39mbatch_size, \u001b[39mlen\u001b[39m(ytrain))])\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(y_pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/encoderrv2.ipynb Cell 9\u001b[0m in \u001b[0;36mencoderTransformer.forward\u001b[0;34m(self, imgArr)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m imgArr:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m   img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(i)[\u001b[39mNone\u001b[39;00m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m   picToVal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisionEncoder(img)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m   to1D \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMaxPool2d((\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m))(picToVal)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m   Arr\u001b[39m.\u001b[39mappend(to1D\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/models/inception.py:166\u001b[0m, in \u001b[0;36mInception3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InceptionOutputs:\n\u001b[1;32m    165\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_input(x)\n\u001b[0;32m--> 166\u001b[0m     x, aux \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(x)\n\u001b[1;32m    167\u001b[0m     aux_defined \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maux_logits\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/encoderrv2.ipynb Cell 9\u001b[0m in \u001b[0;36minception_v3_sliced.<locals>.Inception3Mod._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m       x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(x, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m       x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/encoderrv2.ipynb#W6sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/models/inception.py:405\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 405\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m    406\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x)\n\u001b[1;32m    407\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mrelu(x, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    444\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "def train(model, xtrain, ytrain, xtest, ytest, epochs, batch_size):\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    optimizer = torch.optim.Adam(training_model.parameters(), lr=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(xtrain), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            x = xtrain[i:min(i+batch_size, len(xtrain))]\n",
    "            y = torch.tensor(ytrain[i:min(i+batch_size, len(ytrain))]).float()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_function(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "train(training_model, X_train, y_train, X_test, y_test, 20, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
