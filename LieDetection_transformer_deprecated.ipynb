{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 18:03:24.599457: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Embedding\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dataset\n",
    "import test_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lie_trial_path = './processed_lie/' #60 entries\n",
    "truth_trial_path = './processed_truth/' #61 entries\n",
    "MU3D_path = './processed/' # 300 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no split by person\n",
    "numOfFrames = 10\n",
    "X, Y = dataset.preprocessing(truth_trial_path, lie_trial_path, numOfFrames=numOfFrames)\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "xTrain, xTest = train_test_split(X, test_size=TEST_RATIO, shuffle=False)\n",
    "yTrain, yTest = train_test_split(Y, test_size=TEST_RATIO, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5563, -0.7963, -1.0364, -0.6048],\n",
       "        [-1.8724, -0.7711,  0.3486,  1.2059],\n",
       "        [-0.5719, -0.0841, -0.7283,  0.1213],\n",
       "        [-0.1918, -0.3327,  2.5167,  0.7431],\n",
       "        [-1.0752, -0.6436, -0.8822, -2.4208],\n",
       "        [ 0.4202, -1.5169,  0.0819,  0.5593],\n",
       "        [ 2.0171, -0.4613,  1.5914,  0.2973],\n",
       "        [ 0.5733, -0.1668,  0.6712, -0.5410],\n",
       "        [-2.2083, -0.2195,  0.7277, -0.7213],\n",
       "        [-1.9518, -0.5718,  0.5207,  1.5157]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(num_embeddings=numOfFrames, embedding_dim=4)\n",
    "\n",
    "positions = torch.tensor([0,1,2,3,4,5,6,7,8,9])\n",
    "embedded_positions = embedding_layer(positions)\n",
    "embedded_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "def gen_loc_list(max_frames_count, pos_encoder_size, frames_count = numOfFrames):\n",
    "  embedding = nn.Embedding(max_frames_count, pos_encoder_size)\n",
    "  input = torch.tensor([i for i in range(frames_count)]).clone().detach()\n",
    "  loc_list = embedding(input)\n",
    "\n",
    "  return loc_list\n",
    "\n",
    "def gen_data(data_arr, embedding_arr):\n",
    "\n",
    "  data_arr = torch.tensor(data_arr).clone().detach()\n",
    "  list = torch.cat((data_arr, embedding_arr), 1)\n",
    "\n",
    "  return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "  def __init__(self, inFeatCount, num_encoder_layers, n_heads = 5, n_hidden = 500, dropout = 0.3, outFeatCount = 2): #Out = [prob for 0, prob for 1]\n",
    "    super(TransformerModel, self).__init__()\n",
    "\n",
    "    encoder_layer = TransformerEncoderLayer(inFeatCount, n_heads, n_hidden, dropout)\n",
    "\n",
    "    self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "    self.decoder = nn.Linear(inFeatCount, outFeatCount)\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "      self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "      self.decoder.bias.data.zero_()\n",
    "      self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, data):\n",
    "      encoded = self.encoder(data)\n",
    "      output = self.decoder(encoded)\n",
    "      return torch.nn.functional.softmax(output)\n",
    "  \n",
    "def predict(model, inputArr):\n",
    "    inputArr = inputArr.to(device).float()\n",
    "    \n",
    "    output = model(inputArr)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoder_size = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate embedding\n",
    "embedding_arr = gen_loc_list(numOfFrames, pos_encoder_size)\n",
    "\n",
    "#generate embedded data\n",
    "x_Train = []\n",
    "x_Test = []\n",
    "\n",
    "y_Train = []\n",
    "y_Test = []\n",
    "\n",
    "for i in range(xTrain.shape[0]):\n",
    "    x_Train.append(gen_data(xTrain[i], embedding_arr).detach().numpy())\n",
    "\n",
    "for i in range(xTest.shape[0]):\n",
    "    x_Test.append(gen_data(xTest[i], embedding_arr).detach().numpy())\n",
    "\n",
    "for i in range(yTrain.shape[0]):\n",
    "    if yTrain[i] == 0:\n",
    "        y_Train.append([[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0]])\n",
    "    else:\n",
    "        y_Train.append([[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1]])\n",
    "\n",
    "for i in range(yTest.shape[0]):\n",
    "    if yTest[i] == 0:\n",
    "        y_Test.append([[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0]])\n",
    "    else:\n",
    "        y_Test.append([[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1]])\n",
    "\n",
    "#generate model\n",
    "model = TransformerModel(15, 5).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/pn8653k51_75x5ksbpsy8hm80000gn/T/ipykernel_22055/476567611.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.nn.functional.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 11317.2071, Accuracy: 0.2057, Test Accuracy: 0.2148\n",
      "Epoch 2/100, Loss: 11317.2071, Accuracy: 0.2334, Test Accuracy: 0.2397\n",
      "Epoch 3/100, Loss: 11317.2071, Accuracy: 0.2046, Test Accuracy: 0.2049\n",
      "Epoch 4/100, Loss: 11317.2071, Accuracy: 0.2590, Test Accuracy: 0.2557\n",
      "Epoch 5/100, Loss: 11317.2071, Accuracy: 0.2220, Test Accuracy: 0.2151\n",
      "Epoch 6/100, Loss: 11317.2071, Accuracy: 0.1397, Test Accuracy: 0.1461\n",
      "Epoch 7/100, Loss: 11317.2071, Accuracy: 0.1440, Test Accuracy: 0.1487\n",
      "Epoch 8/100, Loss: 11317.2071, Accuracy: 0.1732, Test Accuracy: 0.1756\n",
      "Epoch 9/100, Loss: 11317.2071, Accuracy: 0.1892, Test Accuracy: 0.2026\n",
      "Epoch 10/100, Loss: 11317.2071, Accuracy: 0.2405, Test Accuracy: 0.2426\n",
      "Epoch 11/100, Loss: 11317.2071, Accuracy: 0.2686, Test Accuracy: 0.2699\n",
      "Epoch 12/100, Loss: 11317.2071, Accuracy: 0.2333, Test Accuracy: 0.2324\n",
      "Epoch 13/100, Loss: 11317.2071, Accuracy: 0.1419, Test Accuracy: 0.1390\n",
      "Epoch 14/100, Loss: 11317.2071, Accuracy: 0.2095, Test Accuracy: 0.2050\n",
      "Epoch 15/100, Loss: 11317.2071, Accuracy: 0.2717, Test Accuracy: 0.2719\n",
      "Epoch 16/100, Loss: 11317.2071, Accuracy: 0.1861, Test Accuracy: 0.1912\n",
      "Epoch 17/100, Loss: 11317.2071, Accuracy: 0.1310, Test Accuracy: 0.1289\n",
      "Epoch 18/100, Loss: 11317.2071, Accuracy: 0.2729, Test Accuracy: 0.2712\n",
      "Epoch 19/100, Loss: 11317.2071, Accuracy: 0.3136, Test Accuracy: 0.3114\n",
      "Epoch 20/100, Loss: 11317.2071, Accuracy: 0.1779, Test Accuracy: 0.1772\n",
      "Epoch 21/100, Loss: 11317.2071, Accuracy: 0.2201, Test Accuracy: 0.2158\n",
      "Epoch 22/100, Loss: 11317.2071, Accuracy: 0.2107, Test Accuracy: 0.2021\n",
      "Epoch 23/100, Loss: 11317.2071, Accuracy: 0.2244, Test Accuracy: 0.2241\n",
      "Epoch 24/100, Loss: 11317.2071, Accuracy: 0.1500, Test Accuracy: 0.1519\n",
      "Epoch 25/100, Loss: 11317.2071, Accuracy: 0.2185, Test Accuracy: 0.2244\n",
      "Epoch 26/100, Loss: 11317.2071, Accuracy: 0.2975, Test Accuracy: 0.2947\n",
      "Epoch 27/100, Loss: 11317.2071, Accuracy: 0.2767, Test Accuracy: 0.2801\n",
      "Epoch 28/100, Loss: 11317.2071, Accuracy: 0.1696, Test Accuracy: 0.1688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mtot_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m         idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m train(model, x_Train, y_Train, x_Test, y_Test)\n",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, xTrain, yTrain, xTest, yTest, epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# update weights\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# evaluate\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    142\u001b[0m            grads,\n\u001b[1;32m    143\u001b[0m            exp_avgs,\n\u001b[1;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m            state_steps,\n\u001b[1;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     94\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m     96\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[1;32m     98\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[1;32m    100\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train(model, xTrain, yTrain, xTest, yTest, epochs = 100, lr = 0.001, batch_size = 10):\n",
    "    \"\"\" Train a model on a dataset \"\"\"\n",
    "\n",
    "    loss_items = []\n",
    "    accuracy_items = []\n",
    "    test_accuracy_items = []\n",
    "    \n",
    "    # create a data loader to handle batching\n",
    "    xTrain_loader =  torch.utils.data.DataLoader(xTrain, batch_size=batch_size, shuffle=False)\n",
    "    xTest_loader = torch.utils.data.DataLoader(xTest, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # create a loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # train\n",
    "\n",
    "        idx = 0\n",
    "        model.train()\n",
    "        for batch in xTrain_loader:\n",
    "\n",
    "            # get data\n",
    "            x_train = batch.to(device).float()\n",
    "            y_train = torch.tensor(yTrain[idx]).to(device)\n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(x_train)\n",
    "\n",
    "            # compute loss\n",
    "            loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "\n",
    "        test_acc = 0\n",
    "        acc = 0\n",
    "        tot_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            idx_test = 0\n",
    "            for batch in xTest_loader:\n",
    "                xTest = batch.to(device).float()\n",
    "                y_pred = model(xTest)\n",
    "                y_test = torch.tensor(yTest[idx_test]).to(device)\n",
    "\n",
    "                # compute test accuracy\n",
    "                test_acc += (y_pred.argmax(0) == y_test).type(torch.float).mean().item()\n",
    "                idx_test += 1\n",
    "                \n",
    "\n",
    "            test_acc /= len(xTest_loader)  \n",
    "            test_accuracy_items.append(test_acc)\n",
    "\n",
    "            idx_train = 0\n",
    "            for batch in xTrain_loader:\n",
    "                xTrain = batch.to(device).float()\n",
    "                y_pred = model(xTrain)\n",
    "                y_train = torch.tensor(yTrain[idx_train]).to(device)\n",
    "\n",
    "                # compute accuracy\n",
    "                acc += (y_pred.argmax(0) == y_train).type(torch.float).mean().item()\n",
    "                idx_train += 1\n",
    "\n",
    "                # computer loss\n",
    "                tot_loss += loss_fn(y_pred, y_train).item()\n",
    "\n",
    "            acc /= len(xTrain_loader)\n",
    "            accuracy_items.append(acc)\n",
    "\n",
    "        # store loss and accuracy\n",
    "        loss_items.append(tot_loss)\n",
    "\n",
    "        # print progress\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {tot_loss:.4f}, Accuracy: {acc:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "        idx += 1\n",
    "            \n",
    "\n",
    "train(model, x_Train, y_Train, x_Test, y_Test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "24b17b925cb255c3f5f9d44279af6acbce2f518dd20c49d83bc1f6775a53d6cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
