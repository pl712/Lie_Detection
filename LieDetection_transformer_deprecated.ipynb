{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:02:55.358512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Embedding\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dataset\n",
    "import test_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lie_trial_path = './processed_lie/' #60 entries\n",
    "truth_trial_path = './processed_truth/' #61 entries\n",
    "MU3D_path = './processed/' # 300 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no split by person\n",
    "X, Y = dataset.preprocessing(truth_trial_path, lie_trial_path, numOfFrames=10)\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "xTrain, xTest = train_test_split(X, test_size=TEST_RATIO, shuffle=False)\n",
    "yTrain, yTest = train_test_split(Y, test_size=TEST_RATIO, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9189,  0.3183, -1.6296, -0.2299],\n",
       "        [-0.1687, -1.0629,  1.4963,  0.7978],\n",
       "        [-1.0853,  0.2260,  0.1388,  1.1185],\n",
       "        [-0.3525, -0.6287,  0.4615, -0.6831],\n",
       "        [ 1.4058, -0.7720, -1.6984,  0.7521],\n",
       "        [ 0.6558,  0.5897, -0.1031,  1.9207],\n",
       "        [-0.7116,  0.8319,  0.7790, -0.1593],\n",
       "        [ 1.3356, -0.0947, -1.7079,  0.2963],\n",
       "        [-0.1453,  0.9592,  0.0719,  0.6384],\n",
       "        [ 1.1880,  1.3794, -0.6667, -1.1403]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(num_embeddings=10, embedding_dim=4)\n",
    "\n",
    "positions = torch.tensor([0,1,2,3,4,5,6,7,8,9])\n",
    "embedded_positions = embedding_layer(positions)\n",
    "embedded_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "def gen_loc_list(max_frames_count, pos_encoder_size, frames_count = 10):\n",
    "  embedding = nn.Embedding(max_frames_count, pos_encoder_size)\n",
    "  input = torch.tensor([i for i in range(frames_count)]).clone().detach()\n",
    "  loc_list = embedding(input)\n",
    "\n",
    "  return loc_list\n",
    "\n",
    "def gen_data(data_arr, embedding_arr, frames_count = 10):\n",
    "\n",
    "  lst = []\n",
    "\n",
    "  for idx in range(frames_count):\n",
    "    data = data_arr[idx].clone().detach()\n",
    "    embedding = embedding_arr[idx].clone().detach()\n",
    "    lst.append(torch.cat((data, embedding), 0))\n",
    "\n",
    "  return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "  def __init__(self, inFeatCount, num_encoder_layers, n_heads = 5, n_hidden = 500, dropout = 0.3, outFeatCount = 2): #Out = [prob for 0, prob for 1]\n",
    "    super(TransformerModel, self).__init__()\n",
    "\n",
    "    encoder_layer = TransformerEncoderLayer(inFeatCount, n_heads, n_hidden, dropout)\n",
    "\n",
    "    self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "    self.decoder = nn.Linear(inFeatCount, outFeatCount)\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "      self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "      self.decoder.bias.data.zero_()\n",
    "      self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, data):\n",
    "      encoded = self.encoder(data)\n",
    "      output = self.decoder(encoded)\n",
    "      return torch.nn.functional.softmax(output)\n",
    "  \n",
    "def predict(model, inputArr):\n",
    "    inputArr = torch.from_numpy(inputArr).float()\n",
    "    inputArr = inputArr.to(device)\n",
    "    \n",
    "    output = model(inputArr)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames_count = 20\n",
    "pos_encoder_size = 5\n",
    "data_arr = xTrain\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/pn8653k51_75x5ksbpsy8hm80000gn/T/ipykernel_1575/2287887436.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.nn.functional.softmax(output)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3312, 0.6688],\n",
       "        [0.4603, 0.5397],\n",
       "        [0.3704, 0.6296],\n",
       "        [0.3317, 0.6683],\n",
       "        [0.4101, 0.5899]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate embedding\n",
    "embedding_arr = gen_loc_list(max_frames_count, pos_encoder_size)\n",
    "\n",
    "#generate embedded data\n",
    "data = gen_data(data_arr[0], embedding_arr)\n",
    "\n",
    "#generate model\n",
    "model = TransformerModel(10, 5)\n",
    "\n",
    "arr = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "predict(model, arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49150, 10, 10)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# training\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, xTrain, yTrain, xTest, yTest, epochs = 100, lr = 0.001, batch_size = 32):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        print(xTrain.shape)\n",
    "    \n",
    "        result = predict(model, xTrain)\n",
    "        loss = criterion(result, yTrain)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(xTrain.shape)\n",
    "    \n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        print(\"Epoch: \", epoch, \" Train Loss: \", train_losses[-1], \" Test Loss: \", test_losses[-1], \" Train Acc: \", train_acc[-1], \" Test Acc: \", test_acc[-1])\n",
    "        optimizer.step()\n",
    "    \n",
    "    return train_losses, test_losses, train_acc, test_acc\n",
    "\n",
    "train(model, xTrain, yTrain, xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "24b17b925cb255c3f5f9d44279af6acbce2f518dd20c49d83bc1f6775a53d6cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
