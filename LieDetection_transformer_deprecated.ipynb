{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 18:03:24.599457: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Embedding\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dataset\n",
    "import test_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lie_trial_path = './processed_lie/' #60 entries\n",
    "truth_trial_path = './processed_truth/' #61 entries\n",
    "MU3D_path = './processed/' # 300 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no split by person\n",
    "numOfFrames = 10\n",
    "X, Y = dataset.preprocessing(truth_trial_path, lie_trial_path, numOfFrames=numOfFrames)\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "xTrain, xTest = train_test_split(X, test_size=TEST_RATIO, shuffle=False)\n",
    "yTrain, yTest = train_test_split(Y, test_size=TEST_RATIO, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5563, -0.7963, -1.0364, -0.6048],\n",
       "        [-1.8724, -0.7711,  0.3486,  1.2059],\n",
       "        [-0.5719, -0.0841, -0.7283,  0.1213],\n",
       "        [-0.1918, -0.3327,  2.5167,  0.7431],\n",
       "        [-1.0752, -0.6436, -0.8822, -2.4208],\n",
       "        [ 0.4202, -1.5169,  0.0819,  0.5593],\n",
       "        [ 2.0171, -0.4613,  1.5914,  0.2973],\n",
       "        [ 0.5733, -0.1668,  0.6712, -0.5410],\n",
       "        [-2.2083, -0.2195,  0.7277, -0.7213],\n",
       "        [-1.9518, -0.5718,  0.5207,  1.5157]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(num_embeddings=numOfFrames, embedding_dim=4)\n",
    "\n",
    "positions = torch.tensor([0,1,2,3,4,5,6,7,8,9])\n",
    "embedded_positions = embedding_layer(positions)\n",
    "embedded_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "def gen_loc_list(max_frames_count, pos_encoder_size, frames_count = numOfFrames):\n",
    "  embedding = nn.Embedding(max_frames_count, pos_encoder_size)\n",
    "  input = torch.tensor([i for i in range(frames_count)]).clone().detach()\n",
    "  loc_list = embedding(input)\n",
    "\n",
    "  return loc_list\n",
    "\n",
    "def gen_data(data_arr, embedding_arr):\n",
    "\n",
    "  data_arr = torch.tensor(data_arr).clone().detach()\n",
    "  list = torch.cat((data_arr, embedding_arr), 1)\n",
    "\n",
    "  return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "  def __init__(self, inFeatCount, num_encoder_layers, n_heads = 5, n_hidden = 500, dropout = 0.3, outFeatCount = 2): #Out = [prob for 0, prob for 1]\n",
    "    super(TransformerModel, self).__init__()\n",
    "\n",
    "    encoder_layer = TransformerEncoderLayer(inFeatCount, n_heads, n_hidden, dropout)\n",
    "\n",
    "    self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "    self.decoder = nn.Linear(inFeatCount, outFeatCount)\n",
    "\n",
    "  def init_weights(self):\n",
    "      initrange = 0.1\n",
    "      self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "      self.decoder.bias.data.zero_()\n",
    "      self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "  def forward(self, data):\n",
    "      encoded = self.encoder(data)\n",
    "      output = self.decoder(encoded)\n",
    "      return torch.nn.functional.softmax(output)\n",
    "  \n",
    "def predict(model, inputArr):\n",
    "    inputArr = inputArr.to(device).float()\n",
    "    \n",
    "    output = model(inputArr)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoder_size = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate embedding\n",
    "embedding_arr = gen_loc_list(numOfFrames, pos_encoder_size)\n",
    "\n",
    "#generate embedded data\n",
    "x_Train = []\n",
    "x_Test = []\n",
    "\n",
    "y_Train = []\n",
    "y_Test = []\n",
    "\n",
    "for i in range(xTrain.shape[0]):\n",
    "    x_Train.append(gen_data(xTrain[i], embedding_arr).detach().numpy())\n",
    "\n",
    "for i in range(xTest.shape[0]):\n",
    "    x_Test.append(gen_data(xTest[i], embedding_arr).detach().numpy())\n",
    "\n",
    "for i in range(yTrain.shape[0]):\n",
    "    if yTrain[i] == 0:\n",
    "        y_Train.append([[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0]])\n",
    "    else:\n",
    "        y_Train.append([[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1]])\n",
    "\n",
    "for i in range(yTest.shape[0]):\n",
    "    if yTest[i] == 0:\n",
    "        y_Test.append([[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0]])\n",
    "    else:\n",
    "        y_Test.append([[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1]])\n",
    "\n",
    "#generate model\n",
    "model = TransformerModel(15, 5).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/pn8653k51_75x5ksbpsy8hm80000gn/T/ipykernel_22055/476567611.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.nn.functional.softmax(output)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m         idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m train(model, x_Train, y_Train, x_Test, y_Test)\n",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, xTrain, yTrain, xTest, yTest, epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# compute loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model(xTrain)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_pred, yTrain)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     loss_items\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/frank/Downloads/Projects/Lie Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb Cell 10\u001b[0m in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(encoded)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/Downloads/Projects/Lie%20Detection/Lie_Detection/LieDetection_transformer_deprecated.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(output)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/transformer.py:202\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    199\u001b[0m output \u001b[39m=\u001b[39m src\n\u001b[1;32m    201\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 202\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/transformer.py:344\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    342\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    345\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    347\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/transformer.py:352\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    351\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 352\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    353\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    354\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    355\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/activation.py:1022\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    971\u001b[0m             need_weights: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, attn_mask: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    972\u001b[0m             average_attn_weights: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Optional[Tensor]]:\n\u001b[1;32m    973\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[39m    query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39m        `batch_first` argument is ignored for unbatched inputs.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1022\u001b[0m     is_batched \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39;49mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m   1023\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1024\u001b[0m         query, key, value \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m (query, key, value)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "# training\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train(model, xTrain, yTrain, xTest, yTest, epochs = 100, lr = 0.001, batch_size = 10):\n",
    "    \"\"\" Train a model on a dataset \"\"\"\n",
    "\n",
    "    loss_items = []\n",
    "    accuracy_items = []\n",
    "    test_accuracy_items = []\n",
    "    \n",
    "    # create a data loader to handle batching\n",
    "    xTrain_loader =  torch.utils.data.DataLoader(xTrain, batch_size=batch_size, shuffle=False)\n",
    "    xTest_loader = torch.utils.data.DataLoader(yTest, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # create a loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # train\n",
    "\n",
    "        idx = 0\n",
    "        model.train()\n",
    "        for batch in xTrain_loader:\n",
    "\n",
    "            # get data\n",
    "            x_train = batch.to(device).float()\n",
    "            y_train = torch.tensor(yTrain[idx]).to(device)\n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(x_train)\n",
    "\n",
    "            # compute loss\n",
    "            loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "\n",
    "        test_acc = 0\n",
    "        acc = 0\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            \n",
    "            for batch in xTest_loader:\n",
    "                y_pred = model(batch.to(device).float())\n",
    "                y_test = torch.tensor(yTest[idx]).to(device)\n",
    "\n",
    "                # compute test accuracy\n",
    "                test_acc += (y_pred.argmax(1) == y_test).type(torch.float).mean().item()\n",
    "                \n",
    "\n",
    "            test_acc / len(xTest_loader)  \n",
    "            test_accuracy_items.append(test_acc)\n",
    "\n",
    "            for batch in xTrain_loader:\n",
    "                y_pred = model(batch.to(device).float())\n",
    "                y_train = torch.tensor(yTrain[idx]).to(device)\n",
    "\n",
    "                # compute accuracy\n",
    "                acc += (y_pred.argmax(1) == y_train).type(torch.float).mean().item()\n",
    "\n",
    "            acc / len(xTrain_loader)\n",
    "            accuracy_items.append(acc)\n",
    "\n",
    "        # store loss and accuracy\n",
    "        loss_items.append(loss.item())\n",
    "\n",
    "        # print progress\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Accuracy: {acc:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "        idx += 1\n",
    "            \n",
    "\n",
    "train(model, x_Train, y_Train, x_Test, y_Test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "24b17b925cb255c3f5f9d44279af6acbce2f518dd20c49d83bc1f6775a53d6cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
